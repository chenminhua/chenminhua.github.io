<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>硕大的汤姆</title><link>https://chenminhua.github.io/</link><description>Recent content on 硕大的汤姆</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 16 Jul 2022 18:00:08 +0800</lastBuildDate><atom:link href="https://chenminhua.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>一些职业思考</title><link>https://chenminhua.github.io/posts/2022_%E4%B8%80%E4%BA%9B%E8%81%8C%E4%B8%9A%E6%80%9D%E8%80%83/</link><pubDate>Sat, 16 Jul 2022 18:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2022_%E4%B8%80%E4%BA%9B%E8%81%8C%E4%B8%9A%E6%80%9D%E8%80%83/</guid><description>昨天和最近带的一个同学 1 on 1（下面简称A），A问我有没有什么建议可以给到他。我想了一会儿，和他说了几点，不一定对，但是希望能帮到他。
ROI 与 push back 刚入职场的工程师往往有个特点，产品提什么需求都愿意接。但是当你成为更高级别的工程师，甚至成为一个方向的owner或者团队leader的时候，你会明白需求是做不完的，你一定要有计算 ROI 的能力，并且能够合情合理地 push back 一些事情。你需要学会把自己和团队的时间放在最值得做的事情上。
当你还是一个初级工程师的时候，其实你做的事情都是被筛选过的有价值的事情，但是当你负责一整个方向的工作时，优先级意识和ROI意识是必备的。这种意识需要从一开始就锻炼起来。
多问“蠢”问题 初级工程师常犯的另一个错误是不敢问问题，比如需求评审的时候，有个地方A不清楚，于是A就想，可以会后再去调研一下。但是更好的办法可能是直接抛出问题。直接抛问题有两个好处：第一是能快速对齐，澄清事实。第二点是建立形象，当你习惯于直接抛出问题后，就会在别人心中建立这样的形象，你是一个会直接抛出问题的人，以后大家在和你合作的时候就会比较放心（不会担心你憋着问题）。
提高 impact 提高 impact 需要你多去帮助别人，而这又需要你对业务和技术有更深的理解。比如你在做某个方向需求的时候，遇到了一个不懂的领域，不要错过它，不要只停留在把需求做完。抓住每一个机会，去了解业务和技术背后的东西，这样当以后别人遇到这个问题的时候，他们都会来找你，当这块需要一个owner的时候，你的leader也会很容易想到你。
不要用工作时长证明自己 A是转岗到我们团队的，他之前的团队据说很卷，大家都习惯于每天十点半以后才下班，他们的leader在绩效打分的时候，也会参考大家的下班时间。因为A当时是应届生，虽然不愿意，但是也和大家一样待到很晚，而转到我们这边的时候，发现我每天都九点前就下班，还觉得很奇怪。
其实我觉得，如果一个团队是用工作时长来衡量你的产出，那说明这个团队压根没什么真正的产出。
更何况，人生除了工作，还有很多重要的事情要做，无论是和朋友建立联系，还是锻炼身体，或者学习以提升自己，甚至就是休息（以达 well being）。工作效率很重要，在完成工作之后，需要为自己争取属于自己的时间，这样才不会 burn out。
和同事搞好关系 The steady state of a successful Internet Century venture is chaos&amp;hellip; Champion racecar driver Mario Andrett: &amp;ldquo;If everything seems under control, you&amp;rsquo;re just not going fast enough.&amp;rdquo; The business should always be outrunning the processes, so chaos is right where you want to be.</description></item><item><title>面试反问问点啥</title><link>https://chenminhua.github.io/posts/2022_%E9%9D%A2%E8%AF%95%E5%8F%8D%E9%97%AE%E9%97%AE%E7%82%B9%E5%95%A5/</link><pubDate>Tue, 12 Jul 2022 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2022_%E9%9D%A2%E8%AF%95%E5%8F%8D%E9%97%AE%E9%97%AE%E7%82%B9%E5%95%A5/</guid><description>如果我加入了，你们会怎么帮助我 ramp up 看看这个团队是不是有明确的 onboarding 机制。如果答得乱七八糟，或者每个面试官说的都不一样的话，小心了，没准你进去第二天就要开始修bug了。
团队的业务是什么，愿景是什么？ 看看这个团队做的事情是不是你感兴趣的。
我个人曾经有过一段失败的工作经历，当时我面上了一个全球top3的公司的团队，我以为我会去做数据库开发，但是入职后发现并非如此，而且工作内容和我的经验与能力并不匹配，尽管团队的同事和文化都很棒，但是四个月后我还是主动提了离职。
很多时候招聘的JD其实都很模糊，你应当在面试过程中尽可能多的了解工作内容的细节。甚至可以问问需求迭代的细节，比如需求是怎么出的，迭代流程是怎么样的等等，这些都和你能否适应这份工作息息相关。
团队面临最大的挑战是什么？当前团队最重要的事情是什么？ 看看这个团队是否有好的规划。很多坑团队其实都没有明确的规划，一旦进去之后可能业务变来变去，你也不能得到很好地沉淀下来做事的机会，工作体验也会很糟糕。
你们最看中工程师身上的什么品质？ 老实说这个问题其实问不出太多，大多数面试官给的都是一些比较空的词。但是可能可以排坑，所以我还是蛮喜欢问的。</description></item><item><title>2022年3月中概暴跌</title><link>https://chenminhua.github.io/posts/2022_%E4%B8%AD%E6%A6%82%E6%9A%B4%E8%B7%8C/</link><pubDate>Mon, 21 Mar 2022 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2022_%E4%B8%AD%E6%A6%82%E6%9A%B4%E8%B7%8C/</guid><description>本文不构成任何投资建议
暴跌原因 导火索：PCAOB(public company accounting oversight board) 把五家中概股公司放入外国公司问责法识别清单，将中美监管摩擦再次推到前台。进而引发市场恐慌。
其他：
滴滴大跌。 国家监管，反垄断，打压互联网公司。美团，阿里，腾讯，教育行业。 俄罗斯乌克兰战争。long only 可能很多都走了。（美国总体资金有 10%在国际市场，哪里有机会去哪里转） 底层 为什么非要去美国上市，而不去香港上市？一个原因是香港流动性太小，第二是美股上市逃避监管，第三是美国上市有品牌效应，第四是美国有比较完整的诉讼体制（事先宽松，事后诉讼）。
中国出公司，美国出体制。但是这个游戏可能未来会玩不下去。
PCAOB 这个事情可能是无解的，所有在美国上市的公司都是要把审计底稿给美国看的，唯一的例外是中国。审计底稿里面都是脏东西，一旦公开会有很大麻烦。
出路 一种可能的发展方向是，公司自查，有问题的退市回香港；没问题的写个保证书然后在美国呆着，但是如果交底稿让人抓住把柄，就会被严惩。（一般来说大公司都有问题）
中国互联网公司上市出路在哪里？只能等一等了。
buy? or sell? 我非常同意中概股依然存在很大的风险。但是我们还是应该聊聊什么是中概的风险。（可以去看我之前的文章「风险与收益」）
毫无疑问我们上面说到的暴跌原因都是风险，但最本质的风险还是价格。
高风险=高价格
最大的投资风险存在于最不容易被 察觉的地方，反之亦然。在所有人都相信某种东西有风险的时候，他们不愿购买的意愿通常会把 价格降低到完全没有风险的地步。广泛的否定意见可以将风险最小化，因为价格里所有的乐观因素都被消除了。 &amp;ndash; Howard Marks
我觉得现在就是价格里面所有的乐观因素都被消除了的时候。
reference https://sv101.fireside.fm/70</description></item><item><title>一生的旅程 -- Iger自传</title><link>https://chenminhua.github.io/posts/2021_%E4%B8%80%E7%94%9F%E7%9A%84%E6%97%85%E7%A8%8B/</link><pubDate>Mon, 21 Mar 2022 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2021_%E4%B8%80%E7%94%9F%E7%9A%84%E6%97%85%E7%A8%8B/</guid><description>Iger 是迪士尼现任 ceo，曾任 abc ceo，后在迪士尼收购 abc 的过程中加入迪士尼，后主导了如收购皮克斯，收购漫威等关键性项目，帮助迪士尼起死回生。Iger 于 2020 年卸任迪士尼 ceo 一职。本文是我今天读完 Iger 自传后的一些摘录。
关于犯错 Iger 在 abc 的第一个老板是一个脾气暴躁的人。有一次在工作中，Iger 犯了个错误，老板很生气，当着所有人的面问是谁把这件事搞砸了。Iger 很羞愧，但还是当着所有人的面承认了错误。老板很吃惊地说：“你居然敢这么做，从来没有人敢承认错误”。Iger 从这件事里面总结了两个经验
在犯错时要敢于承担责任。 管理者要以善待人，用公平和同理心对待每一个人。这不是说犯错没关系，而是要打造出一个环境，让人们知道你是一个会倾听他人讲述事情原委、情绪稳定而处事公平，且会在别人犯下无心之过时给他们第二次机会的人。 关于自知之明 Iger 的第二个老板丹尼斯是个既友善又有趣的人，拥有极具感染力的热情和乐观，最重要的是，他对自己所不懂的事情是有自知之明的。这个特质，在高管身上非常罕见。换作另一个人，若是处在丹尼斯的位置上，便很容易通过伪装权威或常识来对没有电视网工作经验的事实矫枉过正，但这不是丹尼斯的做事风格。在会议期间，大家有时会讨论到某个话题，而丹尼斯不但不会装懂糊弄过去，还会表示自己不懂，并请求我和其他人帮他解释。
提出你必须提出的问题，不带任何歉意地承认自己不懂的东西，做好功课，尽快学到必须学习的东西。
第一条原则，就是不要营造任何假象。你必须保持谦虚，不能把自己佯装成另一个人，也不能不懂装懂。虽说如此，你仍然处在一个领导者的位置上，因此不能让谦虚成了领导他人的绊脚石。这是我在今时今日所宣扬的一个理念，其中的分界线很微妙。你需要提出你必须提出的问题，不要有任何歉意地承认自己不懂的东西，并做好功课，以尽快学到必须学到的东西。没有什么比不懂装懂更能摧毁一个人的自信心了。拥有自知之明，不要假扮别人，这才是真正的权威和领导力的源泉。
我的任务就是不要让自尊心占了上风。我要做的，并不是使尽浑身解数给桌子对面的人留下一个好印象，而是抑制住假装知道自己在做什么的冲动，并多向对方提问。我格格不入，这是无法掩盖的事实。我并没有受过好莱坞的历练，也没有夸张的性格或任何招摇的姿态。
关于迪士尼 Iger 在上任迪士尼 ceo 时，提出了三个战略优先事项。
将绝大多数的时间和资本投入在打造高质量品牌内容上。 在最大限度上拥抱科技，先是利用科技为打造更高质量的产品创造条件，然后再通过更先进和精确的途径来触及更多的消费者。 成为一家真正意义上的全球企业。 毫无疑问，这三点他都做得非常出色。在品牌内容上，他们通过收购皮克斯，漫威等获得了大量优质 IP，并完成了远超预期的开发。在科技方面，他们和苹果有着密切合作，并且也推出了 disney+这些服务。在全球化上，我们也看到了上海迪士尼乐园等一系列发展。</description></item><item><title>不拘一格（网飞的企业文化）</title><link>https://chenminhua.github.io/posts/2022_%E4%B8%8D%E6%8B%98%E4%B8%80%E6%A0%BC/</link><pubDate>Sun, 20 Feb 2022 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2022_%E4%B8%8D%E6%8B%98%E4%B8%80%E6%A0%BC/</guid><description>“人才重于流程，创新高于效率，自由多于管控”
“对于我们的员工来说，透明度代表我们相信员工能够认真负责地对待工作。我们对他们的信任又会增强他们的归属感、使命感和责任感。”
“自由与责任的关系并不是像我先前所想的那样背道而驰，相反，自由是通往责任的一条途径。
人才重于流程、创新高于效率、自由多于管控的企业文化让网飞取得了巨大成功。网飞文化强调以人才密度实现最高绩效，对员工实行情景管理而不是控制，如果一定要说网飞有什么不同之处，那就是它不拘泥于规则。
reed 在 1991 年的时候创办了 pure software，当公司只有十几个人的时候，他们采用了非常自由的管理方式：员工可以在餐厅工作，或者购买办公椅而不需要获得审批。但是这种管理方式也产生了一些问题，由于过于自由，一些员工犯了些愚蠢的错误导致公司受损。面对这些问题，reed 每次都制定了新的规定来防止这些错误再次发生。
但是这些规定和管理流程成了工作的基础，那些擅长在条条框框里循规蹈矩的人得到了提拔，而许多有创造力且特立独行的员工却感到窒息，于是他们便离职去了别处。
由于人才的流失，渐渐地公司创新开始跟不上节奏，虽然效率越来越高，但创造力却越来越弱。为了发展，reed 不得不收购其他拥有创新产品的公司。这就导致业务越来越复杂，规则和流程也越来越多。最终在 1997 年，reed 将公司卖给了最大的竞争对手。
第一，提高人才密度。人才密度越高，你能提供的自由度就越大。 第二，提高坦诚度。 在此基础上，须做好一道减法——减少管控。
首先，将员工手册由厚变薄，差旅、经费支出、休假等相关规定统统可以不要。然后，随着人才密度越来越大，反馈越来越频繁和坦诚，你就可以取消整个组织的审批流程，教会你的经理们“进行情景管理，而非控制管理”，同时让员工把握这样一个原则：工作不是要费心地取悦老板。
最重要的是，一旦你着力营造这样的文化氛围，企业管理便进入了一个良性循环。取消管控将构建一种“自由与责任”（Freedom &amp;amp;Responsibility，简称 F&amp;amp;R）的文化氛围，这也是网飞员工经常挂在嘴边的词。这种文化会吸引很多顶尖的人才，同时又将管控的程度降至最低。实现了这一切，就能让你的公司达到大多数公司无法企及的效率和创新水平。当然，这一切并不是一蹴而就的。
独特的网飞文化
网飞认为你具有惊人的判断力……判断力几乎可以解决所有模棱两可的问题，而流程做不到。
从另一个方面来看……网飞也期待员工发挥出超高的工作水平，不然就得立马走人（不过有一笔丰厚的遣散费）。
第一部分 迈向自由与责任的企业文化 首先，提高人才密度 毫无疑问，具有非凡的创造力、工作出色，且与他人合作良好的员工是留下来的最佳人选。但现在的问题是，许多人都只是在某一方面表现得很好：一些人与同事相处极好，配合默契，但工作能力平平；而另一些人则是工作狂，但缺乏判断力，需要有人引导；同时还存在一小部分人，他们天资卓越，行动力也很强，但是牢骚不断，也很容易产生悲观情绪。他们中大部分人必须离开，但决定让哪些人离开真不是一件容易的事情。
对于优秀员工而言，好的工作环境并不意味着一间豪华的办公室，一个好的健身房，或者一顿免费的寿司午餐，而在于周围全是才华横溢的人，具有合作精神的人，让你不断进步的人。如果每一名员工都很优秀，他们就会相互学习、相互激励，工作表现也会迅速得到提升。
从 2001 年的裁员事件中，里德发现：工作表现无论好与坏，都是具有感染力的。如果你表现平平，可能会影响到很多本可以表现出色的人，导致他们也无心进取。如果你的团队成员个个表现出色，那他们也会相互激励，从而推动彼此取得更大的成就。
其次，提高企业坦诚度 另一项至关重要的，是你在获取反馈时的行为反应。你必须向员工表明，如果你能心怀感激地面对他人的批评，能够给予足够的“认同提示”，那么你也可以放心地提供反馈意见。正如《文化代码》（Culture Code）的作者丹尼尔·科伊尔（Daniel Coyle）所描述的那样，这种认同提示表明“你的反馈将使你成为这个群体中更为重要的成员”，或者“你与我坦诚相待，绝不会对你的工作或我们的关系造成危害。你将得到我们的认同”。我与管理团队经常交流认同提示的问题，因为一名员工就算再有勇气，向领导反馈意见时还是会有担忧。他会想：“领导会不会记仇呢？”“这对我的工作有影响吗？”
认同提示可能只是一个小小的语气或姿势，例如，使用欣赏性的口吻，身体靠说话人近一点儿，用肯定的目光看着说话人的眼睛。当然，你也可以把动作搞大一点，例如，感谢说话者所具备的勇气，并且在众人面前给予赞赏。科伊尔解释说，认同提示的功能是“回答大脑中不断出现的古老问题：我们在这里安全吗？与这些人共处，未来会怎样？有潜在的危险吗？”如果你能通过认同提示对反馈做出回应，员工就会越来越坦诚。
阅读 360 度书面反馈是一件让人感到很刺激的事情。我发现，恰恰是那些最直言不讳的批评是对我最有帮助的。因此，秉持着 360 度反馈的精神，我非常感谢你们勇敢而诚实地给我指出问题，告诉我：“在开会时，如果你觉得讨论话题没有意义或缺乏讨论价值，你可以跳过或者一带而过……同样，不要让你的观点主导了整场会议。你需要协调大家的争论，让大家达成一致的目标。”我感到有些伤心和沮丧，但你们说得太对了，我会继续努力的。希望大家能一如既往地提出和接受建设性的反馈。
4A 反馈准则
提供反馈
目的在于帮助（Aim to assist）：反馈的目的必须是积极的。反馈不是为了发泄，不是为了中伤他人，也不是为自己捞取资本。反馈者应清晰阐述这样做对他人和公司有什么样的好处，而不是对自己有什么好处。“你在与外部合作伙伴会面时在剔牙，这样做很让人生气。”这是错误的反馈方式。正确的反馈应该是这样：“如果在与外部合作伙伴见面时你不再剔牙，那么合作伙伴可能会觉得你很敬业，我们就更有可能建立牢固的关系。” 反馈应具有可行性(Actionable)：你的反馈必须说明接收人可以做一些什么样的改变。我在古巴的那次演讲中，如果收到的是这样一个反馈：“你在演讲过程中的做法与你自己的观点不符。”那这样的反馈就是有问题的。而正确的反馈可以是这样的：“你选取听众发言的方式导致了最后的参与者只有美国人。”或者这样说更好：“如果你还有别的方法，让其他国籍的参会者也发一下言，那你的演讲将更有说服力。” 感激与赞赏(Appreciate)：我们在受到批评时都会为自己辩护或寻找借口，这是人类的本能；我们都会条件反射式地进行自我保护，维护自身的名誉。当你收到反馈时，你需要有意识地反抗这种本能，并且问一问自己：“我该如何去认真地聆听，以开放的心态去认真地对待反馈？既不辩护，也不生气，还应该满怀欣赏和感激。” 接受或拒绝(Accept or discard)：在网飞，你会收到很多人的反馈。你需要认真地听，同时也认真地思考。不是每条反馈都要求你照办，但有必要向反馈者真诚地致谢。你和反馈者都必须清楚：对反馈意见的处理完全取决于反馈的接收者。 当场反馈，实时反馈
► 现在，尝试取消管控……
取消限期休假制度 建立和加强情景管理
自从网飞取消假期跟踪考核之后，其他公司也纷纷效仿，其中包括来自科技行业的玻璃门、领英、Songkick（音乐会推荐应用）、HubSpot（数字营销公司）和 Eventbrite（活动策划平台），还有费舍尔·菲利普斯律师事务所（Fisher Phillips）、高诚（Golin）公关公司，以及电子零售业的 Visualsoft（可视化软件公司）。
给予自由，再落实责任
► 继续尝试取消管控……
取消差旅和经费审批 事前情景设定，事后核实报销</description></item><item><title>服务稳定性建设</title><link>https://chenminhua.github.io/posts/2021_%E6%9C%8D%E5%8A%A1%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%BB%BA%E8%AE%BE/</link><pubDate>Wed, 03 Mar 2021 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2021_%E6%9C%8D%E5%8A%A1%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%BB%BA%E8%AE%BE/</guid><description>第一步：梳理现状 服务的功能有哪些，对线上业务的影响如何。 对服务进行定级，确定是否为重保服务。 确定服务请求量级(峰谷特征)，是否受节日影响，是否受其他接口波动影响。 确定服务部署现状，如是否多机房，是否多集群，是否有灾备预案等等。 梳理上下游，确定服务依赖拓扑，重点确定服务强弱依赖关系。 梳理上下游服务的超时配置，重试配置，熔断，限流配置等。 如果一个下游服务 b 不可用会导致上游服务 a 不可用，则 a 强依赖 b。 如果下游服务 b 不可用会导致上游服务 a 部分功能受影响，但主体功能仍然可以工作，并且业务可短暂接受这种影响，则 a 弱依赖 b。
第二步：问题发现能力 监控报警 检查监控大盘，看是否能覆盖可能出现的问题。常用的监控报警指标包括 服务稳定性 服务拉空率 vv计数 异常指标计数（比如由于某个特定原因对内容进行过滤的量级等） 错误日志计数 RPC调用错误计数 通过调整报警阈值，检查报警规则是否生效，能否准确发出消息或拨打 oncall 同事电话。 容灾演练 模拟故障，通常是模拟网络故障。检查自适应容灾是否生效，以及手动降级预案是否生效。
压测 通过压测寻找资源瓶颈，可能的瓶颈包括
当前压测服务自身资源瓶颈，cpu，内存等。 下游服务资源瓶颈。 依赖存储瓶颈。 如果当前压测服务与其他服务共享下游服务或存储，则可能需要联合压测。
压测可以通过线上流量集中的方案来实现，也可以使用提前录制流量的方案来实现（前提是请求没有副作用）。 如果是写接口，还可以考虑创建影子表等方式来进行压测。
问题定位与处理能力 服务日志要完整。这一点依赖研发工作经验，以及 code review 的负责程度。 内部整理 case 排查指南，问题记录与分享，每周组织学习。 设立 oncall 机制，创建 oncall handbook。 当发现问题时，需要有明确 sop，并通过演练确保所有人都能处理线上事故。 当线上出现事故时，有两种可能，一种是用户反馈发现问题；一种是监控报警发现问题。 无论是哪种情况，首先要对问题严重性有一个基本的判断。（这个也依赖经验和组内分享，比如少量的服务调用报错报警，可能只是网络抖动，但如果用用户反馈说他的账户余额不对，则可能就很严重了） 如果判断出来这是一个需要处理的线上问题，第一步要找同学帮忙，立即成立应急小组，明确分工。比如A同学负责拉群（需要周知到各个职能线），建立会议，并建立事故处理文档；B同学负责定位问题；当然，如果明确知道发生事故前有线上操作（通常都有），比如有代码上线，动态配置，实验开启等，一律先进行线上止损（回滚，或关闭实验）。 事故应急处理负责人需要实时同步事故处理进展。 事故后需要有复盘和todo。 需要有机制check todo落实。 降级开关 容灾预案</description></item><item><title>切尔诺贝利（一键容灾预案引发的事故）</title><link>https://chenminhua.github.io/posts/2021_%E5%88%87%E5%B0%94%E8%AF%BA%E8%B4%9D%E5%88%A9/</link><pubDate>Sun, 03 Jan 2021 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2021_%E5%88%87%E5%B0%94%E8%AF%BA%E8%B4%9D%E5%88%A9/</guid><description>看完了 HBO 的《切尔诺贝利》，真相在最后一集揭晓。
故事中的切尔诺贝利 4 号反应堆，原本应该在建成时就完成的安全测试，在投产后三年都没有完成，并且在事故前已经失败了三次。（容灾演练连续失败三次） 原本应该在白天进行的安全测试，由于基辅电网对产量的要求而推迟，而此时核电站已经半功率运行了半天。（容灾演练随意修改时间） 夜间进行安全测试时，操作员是新人，完全不知道如何操作。（没有事前培训） 安全测试没有通知其他部门的人。（没有对齐其他部门） 半功率运行了半天的反应堆产生了大量的氙气，导致反应堆毒化。 安全测试开始后，需要先降低反应堆功率，而由于反应堆毒化，功率降低过快，很快低于计划的 700mw，并直接降到了 30mw。当值班主任判断出反应堆毒化后，建议先停机后，当时在场负责人佳特洛夫却一意孤行，要求尽快恢复反应堆功率。（发现异常没有立即停止容灾演练） 操作员为了能恢复反应堆功率，将几乎所有控制棒从反应堆内拔出。但由于反应堆毒化，功率仍然没有快速升高，而是维持在了 200mw。 当氙气消耗完后，反应不再受到抑制，功率骤升。当时值班主任阿基莫夫急忙按下 az-5 按钮，试图关停反应堆。（正是这一操作导致了悲剧） az-5 按钮按下后，原本被拔出的控制棒全部同时被插回反应堆。原本这是个「一键容灾预案」，结果由于控制棒的顶端是石墨构成的，石墨会加速反应，这让原本就快要失控的反应堆功率继续上升，引发了爆炸。（容灾预案本身有 bug） 更惨的是，当时的反应堆是单层防护，爆炸后，堆芯直接暴露在空气中，直接伤害了当地居民，并让后续清理工作变得异常困难。(还 TM 没有兜底方案) 当爆炸发生后，加特洛夫还不肯向上反映真实情况，人民不能在最短时间内得到疏散。（没有坦诚面对问题，没有立即止损） 「az-5 容灾预案」的 bug 在三十年前就被发现，但是并没有被严肃对待。（任由线上问题存在） 《切尔诺贝利》是一部非常非常值得观看和反思的作品，有着极大的教育意义。对于软件工程师来说，虽然我们的工作不是操作核电站，但这场人类史上最大的事故也能教会我们很多。
容灾演练应该事前有计划，有安排，有培训，有在线下环境执行过的预案。 容灾演练前要对齐其他部门，避免其他部门由于信息缺失造成恐慌，甚至执行错误的预案或操作。 一旦发现异常立即停止容灾演练，系统优先恢复正常。 系统本身应该有兜底方案，无论是否执行容灾演练，应确保兜底方案有效。 出现事故应该立即止损。 容灾预案不应该有 bug。 发现系统 bug 应该及时同步，并设立 todo 定期检查，查漏补缺。</description></item><item><title>风险与收益</title><link>https://chenminhua.github.io/posts/2020_%E9%A3%8E%E9%99%A9%E4%B8%8E%E6%94%B6%E7%9B%8A/</link><pubDate>Fri, 18 Dec 2020 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2020_%E9%A3%8E%E9%99%A9%E4%B8%8E%E6%94%B6%E7%9B%8A/</guid><description>“高风险带来高收益。”
这句话乍一看，确实是这么回事，比如你今年买了特斯来的股票，起起伏伏间，一年翻了 10 倍，高风险高收益吧；而如果你买了银行定期，自然没什么亏本的风险，但是年化还不到 4%，收益和特斯拉股票一比，差了几百倍，低风险低收益。
但是等等，收益我懂，可什么是风险呢？风险是亏本的可能性？风险是波动的大小？
什么是风险 搞金融的人会用 夏普比率 来描述一个资产在同等风险下的收益能力，其实就是用资产赚钱的期望除以资产的波动。通常你只要记住
夏普比率越高，在相同波动下赚钱的期望越高。
可是，波动本身不会让你亏钱，让你亏钱的只有一件事：高价格。你以高出资产本身价值的价格购买了资产，你亏钱的概率就变高了。这和资产本身的质量无关，只和价格有关。无论你买的是茅台还是苹果的股票，重要的永远是价格。风险并不来自波动，而是来自价格。所以我觉得正确的说法是
风险=高价格。
由此，我们可以进一步推出，当价格上升时，风险在积聚，而当价格下降时，风险在释放（并转化为很多人的亏损）。
即风险在经济上升时增加，并且随着经济失衡的扩大在衰退期化为现实。 即价格上涨风险增加，经济下行，风险变为现实（亏损）并释放。
再进一步，如果「风险=高价格」，而「高价格=低收益」，那我们便可得到一个反直觉的结论：
高风险=低收益
他们不过是同一枚硬币的两面。（我这的是很喜欢说这句话）
风险与收益 其实，「高风险低收益」与「高风险高收益」并不矛盾。只不过，我们对风险的理解并不相同。对于价值投资者来说，风险并不来自于波动，而是来自于高价格。
在牛市中，「高风险高收益」的想法可能更加危险。因为在牛市，投资者容易超过自己能力地去承担过多风险，比如更激进地使用杠杆，或者对高价格失去敏感，更害怕错过「千载难逢的财务自由机会」。
此外，投资者在牛市会降低自己对风险溢价的要求，这导致承担风险所带来的回报补偿降低（风险的价格下降）。简单概况就是：
资产涨价，风险增加，预期收益下降，风险溢价下降。你承担的风险变多，可风险带给你的收益并没有变多。
什么时候风险最大？ 当大家都认为风险已经消除的时候。所有人都坚定地认为该资产没有风险，于是，风险溢价降低，购买意愿上升，大家会用更激进地态度来对待这个资产，反而让风险不断扩大。
更好的安全装备可能吸引登山者承担更多的风险 ——实际上会令他们更不安全。
最大的投资风险存在于最不容易被 察觉的地方，反之亦然。在所有人都相信某种东西有风险的时候，他们不愿购买的意愿通常会把 价格降低到完全没有风险的地步。广泛的否定意见可以将风险最小化，因为价格里所有的乐观因素都被消除了。 &amp;ndash; Howard Marks</description></item><item><title>RocksDB</title><link>https://chenminhua.github.io/posts/2020_rocksdb/</link><pubDate>Tue, 10 Nov 2020 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2020_rocksdb/</guid><description>认识 RocksDB RocksDB 是什么? 一个嵌入式的 lsm style 的 kv 存储引擎，一个 c++的库。 key 和 value 都是 byte streams，key 按序存储。 高度可定制，能通过组装不同模块的方式适应不同类型的磁盘，负载。 能够通过增加 cpu core 以及 IOPs 进行线性 scale。 为 flash 和内存做优化。 RocksDB 不是什么？ rocksdb 没有 server code，用户需要自己实现 server 的部分来得到 c-s 架构的数据库。 RocksDB 不是分布式数据库，没有 fault-tolerance 和 replication 机制，也没有实现 data-sharding。 Not distributed，No failover.No HA, if machine dies you lose your data. 为什么需要 RocksDB 基于 flash 存储的 ssd 普及，网络 latency 在 query workload latency 中占据的比例越来越高。embeded 数据库变得受欢迎。 dhruba 尝试比较 HBase/HDFS 和 mysql 在 query serving workload 上的表现。经过多次优化后，在机械硬盘上，几 pb 的数据集下，hbase 可以达到比 Mysql 慢两倍的查询速度。但是在 ssd 上，hdfs/hbase 等 hadoop 生态存储在当时还不能高效利用 flash，因此 dhruba 开始试图探索新的存储引擎。（dhruba 开始试图扩展 hdfs/hbase 的能力，使其能 serve query workload。但是随着 flash 的普及，他发现 hdfs 对 flash 的使用效率不高。并且将 hdfs/hbase 改成嵌入式的难度太高，因此他决定开发新的数据库存储引擎。） 当时市面上有一些嵌入式数据库，leveldb 是其中的佼佼者。http://leveldb.</description></item><item><title>限流与过载保护</title><link>https://chenminhua.github.io/posts/2020_%E9%99%90%E6%B5%81%E4%B8%8E%E8%BF%87%E8%BD%BD%E4%BF%9D%E6%8A%A4/</link><pubDate>Thu, 22 Oct 2020 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2020_%E9%99%90%E6%B5%81%E4%B8%8E%E8%BF%87%E8%BD%BD%E4%BF%9D%E6%8A%A4/</guid><description>过载保护 微信的这篇论文有个很好的介绍
过载控制往往需要对不同的服务进行专门设计，但是这种过于细致的过载控制对于整个系统来说是不利的，开发者很难估计每个服务应该可以承受的负载。
为此，微信采用了一种名为 DAGOR 的过载控制设计，在 service 粒度上管理过载，使得每个微服务的负载能被实时监控。
由于服务之间复杂的依赖关系，比如一个请求依赖 k 个服务，而这些服务都有 p 的概率拒绝服务，那么这个请求成功的概率就是 (1-p)^k。如果 k 比较大的话，服务的 sla 会明显下降。
DAGOR DAGOR 的基本机制是：当请求到达时，为其分配一个业务优先级和红黑优先级，根据这个优先级使得它下游的服务可以强制接受或者拒绝这个请求。每个服务都有一个自己的优先级阈值，并根据其检测到的当前负载情况调整阈值。
过载检测 另一个问题是，服务如何判断自己是否处于过载状态？DAGOR 采用等待队列里的平均等待时间（简称排队时间（queuing time））来检测负载状况。
排队时间 = 请求开始被处理的时刻 - 请求到达此服务的时刻。
为什么不用 response time 或者 cpu 利用率 来检测负载情况呢？
对于一个服务来说，responese time 不止收到其自身处理请求速度（自身负载）的影响，也受下游服务处理时间的影响。 cpu 利用率即使很高，但如果请求能被及时处理，我们也不应该认为服务进入过载状态。 而 queuing time 则仅受本服务处理能力的影响。如果一个服务有足够的资源来处理请求，queuing time 就会很小，及时下游返回慢，本服务 queuing time 也不会被影响。 DAGOR 的过载检测是基于窗口的，比如在微信系统中，每秒或者每 2000 个请求，server 就会刷新它的监控状态。微信中平均的 queuing time 阈值为 20ms，超过了就意味着服务过载了。
限流 单实例限流 简单的令牌桶就可以实现，可以基于 mesh 实现，或者基于服务框架实现。
分布式限流 常规手段还是使用 redis 计数，但是由于热 key 的问题，一般只适合于 qps 有限（小于 1w）的场景。 如果是高 qps 场景，通常都不需要精准限流，可以考虑退化到单实例限流模式。 如果是高 qps 并且需要精确限流，方案比较复杂。 key = fmt.</description></item><item><title>geohash</title><link>https://chenminhua.github.io/posts/2020_geohash/</link><pubDate>Mon, 12 Oct 2020 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2020_geohash/</guid><description>今天你想吃火锅，于是你问 siri，&amp;ldquo;离我最近的火锅店有哪些？&amp;quot;。
怎么实现这个功能呢？首先你应该抽象地想想，这是一个什么问题？
这是一个「二维空间查找最近邻」问题，而「最近邻」又比较容易让人想到”KNN“。（KNN 是说每个新样本的分类可以用离这个样本最近的 K 个样本的分类来表示，而我们这里要做的并不是一个分类问题，但是 knn 通过哪些手段来找到最近的 k 个样本&amp;ndash;尤其是在样本量很大的时候，则非常值得参考。比如说 kd-tree 之类的）。
但是如果我们更多地往「查找」上面想，我们就会想到「二分」，想到「查找树」，想到「哈希」。
先来简化一下这个问题，「一维空间查找最近邻」怎么弄？
这个问题我们都会，比如把所有元素存在一个有序数组中，然后二分查找，查找的时间复杂度为 O(log n)。数组虽然查找快，但是它的缺点是插入、删除、更新的速度太慢了，如果我们希望在保持查找速度的同时提升插入、更新、删除的速度，就要用到树。
那如果现在我们有一个二维空间，怎么建树呢？kd-tree 就是一个建立多维树的方法。但是我们还有别的方法，我们可以想想，怎么把二维空间变成一维。也就是说我们寻找一个函数。
f(x, y) -&amp;gt; z 这个函数必须满足以下特性，假设我们有(x1, y1) 和 (x2, y2)，如果 x1 约等于 x2，并且 y1 约等于 y2，则 z1 约等于 z2；只要 x 或者 y 两个特征中有一个相差很大，z 就必须相差很大。也就是在二维空间接近的两个点的 z 要接近，而在二维空间上距离远的两个点的 z 要远。
GEOHASH 完美的解决了这个问题。GEOHASH 的算法其实非常简单，就是将地理位置的经度和纬度分别二进制化，然后交叉编码。比如一个地理位置的经度编码为 10000，纬度为 10111&amp;hellip;，则编码后得到的 geohash 为 1100010101，然后再将这个二进制数进行 base32 编码，比如这里得到的就是 SP&amp;hellip;。
现在假设有三个地址，编码是 SP456，SP457，45FP3，显然 SP456 和 SP457 离得比较近。我们成功的将「二维空间查找最近邻」的问题变成了「字符串前缀匹配」的问题。而字符串前缀匹配的问题就简单了，构造一个树做索引就行了。</description></item><item><title>不要在错误的位置和错误的人发生错误的竞争</title><link>https://chenminhua.github.io/posts/2020_%E7%AB%9E%E4%BA%89/</link><pubDate>Sat, 23 May 2020 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2020_%E7%AB%9E%E4%BA%89/</guid><description>程序员行业是一个准入门槛极低但竞争非常激烈的行业，但高级程序员是一个准入门槛不高但竞争非常不充分的行业。尽管我还没有达到高级程序员的水准，但据我的观察，我觉得从初级程序员到高级程序员的路，并不坎坷，只是很多人都走歪了。
记得《Becoming Warren Buffett》里面，巴菲特说的，
成功的关键在于玩自己擅长的游戏。
好胜心能帮助我们成功，但有时也会害了我们。对于那些从小的“聪明人”，他们受好胜心的“伤害”更大。因为他们在任何竞争中都想要赢，也就更容易忽略这个游戏是不是自己擅长的游戏，该不该玩这个游戏。在职场中，每个公司都有自己的企业文化和规则，就好像游戏的规则一样。我们很容易进入努力适应规则的状态，以成为这个游戏中的赢家，却忘了自己其实可以选择不玩这个游戏。
而真正的高端玩家应该要先能够分辨出，自己该坐上哪个牌桌，而不是直接上桌玩牌。
Before the game start, Know your game.</description></item><item><title>隔离 -- 我们都是这疯狂世界的受害者</title><link>https://chenminhua.github.io/posts/2020_isolation/</link><pubDate>Fri, 07 Feb 2020 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2020_isolation/</guid><description>John Lennon《Isolation》的最后写到「你不必受到责怪，你也不过是一个普通人类，一个疯狂世界的受害者。我们害怕每一个人，害怕阳光。太阳永远不会消失，可留给这个世界的时间已经不多了。」
People say we got it made,
don&amp;rsquo;t they know we&amp;rsquo;re so afraid?
Isolation.
We&amp;rsquo;re afraid to be alone,
Everybody got to have a home,
Isolation.
Just a boy and a little girl,
Trying to change the whole wide world,
Isolation.
The world is just a little town,
Everybody trying to put us down,
Isolation.
I don&amp;rsquo;t expect you to understand,
after you&amp;rsquo;ve caused so much pain.
But then again, you&amp;rsquo;re not to blame,</description></item><item><title>事务</title><link>https://chenminhua.github.io/posts/2020_%E4%BA%8B%E5%8A%A1/</link><pubDate>Thu, 16 Jan 2020 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2020_%E4%BA%8B%E5%8A%A1/</guid><description>尽管大多数程序员都认为事务是如此简单和自然，但事实上事务不是一个天然的东西，而是人为创造出来的，目的是为了简化应用层编程。
对应用层来说，底层可能出现的错误实在太多了。网络可能中断，数据库软件可能崩溃，应用程序自身可能突然崩溃，机房可能断电，数据更新可能被覆盖导致丢失，或者读到一些部分更新的数据。而事务就是一种将一系列操作捆绑成一个原子操作（一起成功或一起失败），并对外提供隔离性和持久性，从而将复杂的并发读写问题从应用层抽离到数据库层面的技术手段。
换句话说，即使没有事务，应用层有时也能工作。但是没有原子性保证后，操作的中间状态会多的让人崩溃，错误处理也会异常复杂。而如果没有隔离性，则会出现各种并发导致的数据覆盖导致更新丢失，或者读到中间状态数据的问题。
事务提供了什么 ACID，即原子性，一致性，隔离性，持久性。
原子性：在出错时中止事务，并将部分完成的写入全部丢弃。也许可中止性比原子性更为准确。 一致性主要只对数据有特定的预期状态，例如对账单系统来说要保证账户金额平衡。事实上，一致性更多是靠应用层来保证的，应用程序需要依靠数据库提供的 AID 来达到一定的 C。（Joe Hellerstein 曾经说过，C 只是为了让 ACID 这个缩略词读起来更顺口，lol） 隔离性意味着并发执行的多个事务相互隔离，不能相互交叉，虽然实际上他们可能同时运行，但是数据库要保证数据提交时，其结果与串行运行一样。 持久性意味着一旦写入成功，即使数据库崩溃，数据也不能丢。 除了 ACID 之外，存储系统有时候还会提供一些高级功能，比如当事务失败之后，究竟应该如何表现？一个简单的方案是直接丢弃所有请求。而有些无主节点的分布式存储系统则会采取“尽力而为”的策略，尝试多做一些工作。还有一些系统会采用在应用层重试事务的策略，这需要格外小心重复写的问题。在分布式系统中，错误处理是非常尴尬的事情，原因在于你对错误的了解是不完备的，除非对方系统能明确告知错误类型，否则你很可能无法得知真正的错误类型。
另外还要补充一点，有些时候逻辑上的事务可能会跨存储，甚至会有不可逆的副作用（比如发短信，当你发现事务无法提交成功而必须回滚，可是消息却已经发出去了）。对于这种跨系统的事务，我们可以采用补偿机制，两阶段提交机制等等方式来实现分布式事务。
弱隔离级别 前面我们提到，隔离性要保证并发执行的多个事务相互隔离，不能交叉。首先我们应当澄清，当两个事务需要读写的数据完全没有交集的时候，即使他们是同时执行的，也不会导致并发问题（尽管有时候我们认为负载问题也是并发问题，但这里我们认为他们完全是两码事）。
一种实现隔离的方式是串行执行，有时候，这可能会是个好主意（比如对于 redis 这种基于内存的 kv 数据库），但对于大多数关系型数据库应用场景来说，串行带来的性能损失是不可接受的。更重要的是，这种完全拒绝并行的方案非常危险，因为只要有一个事务被卡住了，其他事务将完全无法工作。
因此，数据库通常会提供串行化之外的其他相对较弱的隔离级别。弱隔离级别会带来一些并发问题，甚至已经造成了大量损失，但是在并发性的诱惑下，这些弱隔离级别得到了广泛应用。
读-提交 读提交是最基本的事务隔离级别，它提供了两个保证：只能读到已经提交的数据（防止脏读）；只能覆盖已经提交的数据（防止脏写）。
防止脏读的一种策略是，对于每个待更新的值，数据库都会维持新值和旧值两个版本。事实上支持快照隔离级别的存储引擎会直接采用 MVCC 来实现读提交。
而对于脏写的问题，数据库通常采用行级锁来实现，当事务想要修改某行时，它必须首先获得行锁，并持有到事务提交。如果有另一个事务也想改这行，则必须等待。这种锁定机制是读提交模式下数据库自动完成的。
读倾斜 读提交会带来一些数据不一致的问题，我们来看下面这个例子。
假设 Alice 在银行有 1000 美元存款，分两个账号，各 500 美元。然后她从账号 1 转了 100 美元到账号 2。但是在转账的过程中，她有可能在查自己账户的时候，会发现自己只有 900 美元了。
trx1: select balance from accounts where id = 1; 返回 500 美元。 - 下面事务 2 开始转账 trx2: begin; trx2: update accounts set balance = balance + 100 where id = 1; trx2: update accounts set balance = balance - 100 where id = 2; trx2: commit; - 事务 2 转账结束，并提交 trx1: select balance from accounts where id = 2; 返回 400 美元。 这种现象就是读倾斜(read skew)，也叫不可重复读。在有些场景下，不可重复读不是什么大问题，但是有时候，可能会带来灾难。</description></item><item><title>软件随想录(by Joel Spolsky)</title><link>https://chenminhua.github.io/posts/2020_%E8%BD%AF%E4%BB%B6%E9%9A%8F%E6%83%B3%E5%BD%95/</link><pubDate>Sat, 11 Jan 2020 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2020_%E8%BD%AF%E4%BB%B6%E9%9A%8F%E6%83%B3%E5%BD%95/</guid><description>这周在地铁上读完了 Joel Spolsky 的《软件随想录》，译者是阮一峰。摘录或提炼一部分有趣的东西于此。
1991 年，Joel Spolsky 加入微软 Excel 开发小组，担任 Program manager。他当时需要为 Excel 的宏寻找一个解决方案（其实就是 basic 语言内嵌到 excel 中）。Joel 为这个功能编写了 500 页的规格说明书，并打印出来交给 Bill Gates 审查。令 Joel 惊叹的是，Bill 用一个晚上就看完了 500 页说明书，并写满了备注的问题，同时在审查会议上，提出了各种非常专业的技术问题。Joel 说，「你不能糊弄 bill，哪怕是一分钟，因为他也是一个程序员，一个真正的程序员」。
同所有行业的人才一样，优秀的程序员不会出现在招聘市场上。一个推论是，在人才市场上找工作的人，大部分是一些水平很差的人（真正牛逼的程序员不需要找工作，他们可以去任何他们想去的地方，而雇佣他们的人如果不是太蠢，也会想尽办法留住他们）。而想要得到那些真正的人才，除了自身牛逼之外，还有三个方法。第一，去牛逼大学里面找牛逼学生，给他最好的实习机会。第二，去优秀的技术社区找，尤其是那些极客扎堆的。第三，建立自己的社区以吸引人才。
好的管理不应该只是告诉别人做什么，也要告诉他为什么要这么做，以获得对方的认同。
Joel 说，「我从来没见过哪个能用 scheme 语言、haskell 语言和 c 中的指针写代码的人，不能在两天内学会 java」。常春藤院校只教 unix，函数式编程、状态机理论，而顺着学校排名往下看到一些比较差的学习，java 语言开始在课程中出现了。（lol）
在一个研究型大学里，系主任实际上是一种负担，没有人真的想干，大家都更愿意去做研究。硅谷式的管理风格正是这样。经理存在的唯一理由就是把家具的位置摆好，不要挡道。这样天才才能做出优秀的成功。
那些决定游戏规则的人都是善于写作的人，为什么 C 语言是最流行的语言，原因是创始人 Brain Kernighan 和 Dennis Ritchie 写出了一本伟大的书。（《The C Programming Language》确实是一本非常非常棒的书，我至少看了三遍）
最有权势和影响力的程序员正是那些表达能力强的程序员，无论书面还是口头，他们都能清晰、自如、有说服力地表达观点。
为了发现可以改进的地方，你必须有一个思维定势，始终用批判的眼光看世界。随便找一样东西，如果你看不出它的缺点，那么你的思维转型还没有成功。
眼睛的工作原理与内存访问的 page-fault 机制有类似之处。
「说实话，我觉得“杜绝信息孤岛”对“架构太空人”最有吸引力。那些人看到子类就会想到抽象的基类，他们喜欢把功能从子类移到基类中，但是又说不出实际好处，唯一理由就是这样符合软件架构上的美学」。
别给用户太多选择。太多选择实际上损害了我们内心的幸福感。（Barry Schwartz 《The Paradox of Choice: Why More is Less》）</description></item><item><title>Haystack(facebook是怎么存照片的)</title><link>https://chenminhua.github.io/posts/2019_haystack/</link><pubDate>Wed, 25 Dec 2019 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2019_haystack/</guid><description>本文写于 21 世纪 10 年代最后一个圣诞节的晚上，内容为 facebook 的论文 《Finding a needle in Haystack: Facebook’s photo storage》的阅读笔记。该论文旨在解决社交网络中海量小文件且请求具有长尾特性的服务能力问题。
简单来说，传统文件系统会给每个文件建立 metadata，比如 inode 这种数据结构，在访问文件的时候，需要先从磁盘找到 inode，然后从 inode 里面查到文件真正的位置，再去拿文件，于是 metadata lookup 就成了瓶颈。而 haystack 所做的就是尽可能减少磁盘访问。
当然，那文件的那次磁盘 IO 是跑不掉的，那有没有可能省掉 metadata lookup 这一此磁盘 IO 呢？有点难，因为要处理海量的小文件，他们的 metadata 也是非常非常大的，内存肯定放不下。所以思路就到了怎么压缩 metadata，以使其能够放入内存中去。
具体是怎么做的呢？卖个关子，去我的知乎专栏看吧~ https://zhuanlan.zhihu.com/p/99388774</description></item><item><title>LSM Tree vs B-Tree</title><link>https://chenminhua.github.io/posts/2019_lsm_tree_vs_btree/</link><pubDate>Tue, 24 Dec 2019 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2019_lsm_tree_vs_btree/</guid><description>本文为《数据密集型应用系统设计》第三章第一节的读后感
数据需要持久化，将内存中的状态落到磁盘上，就需要使用存储引擎。最简单的存储引擎就是一个数据文件呗，每次写入就写到文件上，而读操作就去数据文件上找数据。但是数据量大了，把所有文件找一遍就不靠谱了，需要加索引。我们就先从最简单的索引，hashmap 说起。
从 hashmap 说起 hashmap 就是一种特别简单的索引，其具有非常好的读写性能，但是有一个缺点，需要全部加载到内存中。虽然我们也可以将 hashmap 序列化后落盘，但是其实现查询功能还是必须整个加载。在数据量比较大的情况下，这显然是个大问题。
Riak 中的默认存储引擎 Bitcask 就是采用的 hashmap 作为索引。Bitcask 将数据加载到文件中，并在内存中维护 hashmap，记录每个 key 对于的 value 在文件中的偏移。这样每次查找的时候，只需要按照 Key 找到数据所在的文件和偏移，就能找到数据了。这种方式其实非常常见，文件系统很多都是这么干的。
当然这只是一个粗略的解释，实际的实现远比这复杂。
比如，都写到一个文件里，那如果这个文件坏了怎么办，如果并发很高怎么办？所以，应该拆成多个段文件。
比如，如果要更新数据怎么办？是直接回到原来的地方去更新么？那就失去了顺序写的巨大性能优势了。更好的方式是先一股脑追加写，不但可以顺序写，这对崩溃恢复也更友好（因为可能更新一半的时候挂掉）。
但是追加写不是很浪费空间么？这就需要做事后合并了。（合并的时候显然还是要更新索引的，并且合并也导致了一定的「写放大」，写放大就是一次数据库写导致了多次的磁盘写）
如果要删除数据怎么办？其实和更新类似，应该先写在原来的数据上打一个墓碑标志（删除标志），然后在合并数据文件的时候，将其跳过。
LSM-Tree (SSTable) 如果我们要解决 hashmap 索引必须加载到内存中的问题，我们需要找到一种基于磁盘的索引机制。
首先我们来看 SSTable (sorted string table)，其要求文件上的数据的 key 要有序，并且在合并文件中，相同的键只能有一条记录。有序性带来一个很大的好处，我们只需要很少量的索引，就可以快速确定某个文件里面有没有待查找的键的对应的值，以及如果有的话在什么区间了。
但是写入磁盘这种事情，要做到在一个文件上有序是很困难的，因为写入会以任意顺序发生。所以顺序还是要在内存中维护。一种常见的做法是，写入时现在内存中维护一个有序结构（比如红黑树）。当红黑树比较大了以后，将其写入磁盘。
很明显，这种做法存在一个问题，就是如果在写入磁盘前崩溃，内存里的数据就丢了。解决这个问题的方法通常是增加一个 WAL 文件，即先将记录写到磁盘上（这里是顺序写），在机器从崩溃中恢复的时候用于恢复数据。
LevelDB 和 RocksDB 正是使用了这种算法，类似的还有 Cassandra 和 HBase。
由于最初这个算法被称为 (Log-Structured Merge Tree)，后来这种基于合并和压缩排序文件的存储引擎就被称为 LSM 存储引擎。
我们可以想象一下 LSM 的最坏场景，查找一个不存在的键，这几乎就是灾难，因为几乎要遍历所有的数据文件（或者说数据的索引文件）。一个解决方案是用布隆过滤器来判断数据是否存在。布隆过滤器的特点是，如果存在则不会误判，如果不存在则可能误判，即存在假阳性错误，这不会导致数据读取错误，只会在假阳性发生时带来性能上的损失，还好，假阳性不常发生。
BTree 事实上，BTree 才是数据库所有的真正王者，几乎是关系数据库索引的标准实现。
LSM Tree 是日志结构的，它将磁盘看成可变大小的块，每个块为几兆或更大，并始终按照顺序入段。
而 BTree 则将磁盘看成固定大小的页，页是读写的最小单元。可以说，这种设计更接近硬件。一个页面指向其他的一些页面，并形成一个高扇出（出度高）的树状结构。</description></item><item><title>隐马尔科夫链与维特比算法</title><link>https://chenminhua.github.io/posts/2019_%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE%E4%B8%8E%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95/</link><pubDate>Sat, 21 Sep 2019 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2019_%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E9%93%BE%E4%B8%8E%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95/</guid><description>马尔科夫链 明天的天气怎么样？明天的股市怎么样？用户下一个输入的单词会是什么？这些问题都是一个随机过程的问题（就是「随机过程随机过」的那个随机过程）。
对随机过程的研究要比随机变量复杂得多，在任何一个时刻 t，对应的状态 st 都是随机的，而我们要研究不同时刻的状态之间的相关性就变得非常复杂。比如今天的最高气温可能和之前几天的最高气温都是相关的，这让模型的建立变得复杂。马尔科夫为了简化问题，提出了一种假设，即
随机过程中每个状态的概率分布只和其前一个状态有关。
注意，我们不能否认前天的天气和今天天气的相关性。马尔科夫链并不是直接否认前一时刻的状态与后一时刻状态之间的相关性，而是说明了在当前时刻状态已知的前提下，下一时刻的状态只与当前时刻有关，而和之前的时刻无关。
隐马尔科夫模型(HMM) 马尔科夫模型极大得简化了随机过程的研究，但是在现实问题中，我们常常无法直接获得随机过程中各个时刻的状态。比如语音识别，当你听到“li hai”，可能是在说「厉害」，也可能是在说「里海」。听到的“li hai”是观测到的现象，而背后的信息才是我们要研究的随机过程的状态。这就是隐马尔科夫模型
任意时刻 t 的状态 st 是不可见的，观测者不能直接观察状态序列来推测状态转移模型的参数。但是隐马尔科夫模型在每个时刻都会输出 ot,而且 ot 仅和 st 相关（独立输出假设）。
状态转移矩阵 与 发射矩阵 如果 HMM 模型的状态是离散的，而观察状态也是离散的，我们就可以得到两个关键的矩阵。一个是状态转移矩阵，其表示模型在不同状态间跳转的概率。比如状态为「不感冒」和「感冒」，则[[0.7, 0.3], [0.4, 0.6]]表示
如果昨天不感冒，则今天 70%不感冒，30%感冒 如果昨天感冒了，则今天 40%不感冒，60%感冒 另一个矩阵叫做发射矩阵，其表示在不同的状态下产生不同观测值的概率，比如
如果不感冒，则有 50%的概率表现正常，有 40%概率觉得冷，有 10%的概率感觉头晕 如果感冒了，则有 10%的概率表现正常，有 30%概率觉得冷，有 60%概率觉得头晕 除此之外，我们还有一些先验知识。比如人有 60%的概率处于健康状态，有 40%的概率处于感冒状态。
请问，假设我第一天感觉正常，第二天感觉有点冷，第三天感觉有点 dizzy，我这三天最可能的状态分别是什么呢？
维特比算法 这个问题可以被建模为图遍历的问题。也就是从第一天开始一直到今天，状态流转的路径中可能性最大的路径。而没过一层的可能性都有「状态数的平方」那么多，所以整个图求解也是复杂度非常高的。
而维特比算法就是解决这个问题的一种动态规划算法。其核心思想在于
到达今天各个状态的最大概率，是由到达昨天的各个状态的最大概率，与今天的观测情况决定的。
比如今天感觉有点冷吧，如果昨天身体好的很，那么今天很可能并没有感冒，但是如果昨天就感冒了，那今天感冒的概率就很大。
那么在感觉冷的情况下今天感冒的概率是多少呢？
我们要知道昨天感冒的概率，昨天感冒且今天感冒的转移概率 我们要知道昨天没感冒的概率，昨天没感冒且今天感冒的转移概率 我们要知道感冒导致感觉冷的概率 P(今天感冒) = 常数 * max(P(昨天感冒)*P(今天感冒|昨天感冒)*P(觉得冷|感冒), P(昨天没感冒)*P(今天感冒|昨天没感冒)\*P(觉得冷|感冒)) 同样我们也可以算出今天没有感冒的概率。通过动态规划，我们可以算出整个随机过程的 path 下最可能的状态转移路径。
(小知识：维特比算法的发明人安德鲁维特比在 1985 年创立了高通公司。)</description></item><item><title>P99与蓄水池算法(reservoir sampling)</title><link>https://chenminhua.github.io/posts/2019_p99%E4%B8%8E%E8%93%84%E6%B0%B4%E6%B1%A0%E7%AE%97%E6%B3%95/</link><pubDate>Wed, 04 Sep 2019 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2019_p99%E4%B8%8E%E8%93%84%E6%B0%B4%E6%B1%A0%E7%AE%97%E6%B3%95/</guid><description>一个监控问题 假设你是一个运维工作人员，维护着一个访问量巨大的服务，然后有一天，老板跑来问你这个服务的 p99 响应时间是多少？（p99 响应时间：系统 99%的请求都快于这个时间，而 1%的请求则慢于这个时间。即响应时间的 99% 分位点。）
一个简单的算法是直接对所有请求的访问时间进行排序，然后取出 99%位置的请求的访问时间。但是由于系统访问量巨大，每天都有上亿的请求，而一般的排序算法的时间复杂度为 O(n * log(n))，在这样的数据量级上，算法的空间复杂度也很高。怎么优化这个算法呢？
抽样 上亿的数据实在太多了，但是假设我们的系统是相对稳定的，则所有数据应该是同分布的，那我们应该可以通过随机采样的方式，选出一部分样本来，用这些样本来描述总体。比如说我知道今天会有一亿个请求，然后我就可以按千分之一来进行数据抽样，即每个数据都按照千分之一的概率抽取到样本集中。
这里就遇到另一个关键性的问题，我不知道今天会有多少请求，即我不知道总体有多大，更讨厌的是，总体在不断变大，那样本集也会不断变大，排序会越来越困难。
这时候，你应该打开 John Bentley 所著的《编程珠玑》，翻到第十二章，那里有这个问题的答案。
答案就是蓄水池算法，具体可参考《编程珠玑》或 wiki。https://en.wikipedia.org/wiki/Reservoir_sampling , 算法如下。
for (long i = 0; i &amp;lt; LOOP_NUM; i++) { if (i &amp;lt; SAMPLE_NUM) { samples.push_back(n(e)); } else { long r = randint(0, i); float ne = n(e); if (r &amp;lt; SAMPLE_NUM) { samples[r] = ne; } } } 注意，这个 samples 数组是动态更新的，每个新样本进来都会被随机选择是否替换进来。
排序 回到上面的 p99 问题，我们可以设定程序每隔一段时间对样本数组进行排序，并选择 99%位置的数字。时间复杂度就是 O(m * log(m))，空间复杂度为 O(m)，其中 m 为样本集大小。</description></item><item><title>Cuda实战入门2: 将矩阵乘法速度提升 5000 倍</title><link>https://chenminhua.github.io/posts/2019_cuda%E5%85%A5%E9%97%A82/</link><pubDate>Sat, 24 Aug 2019 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2019_cuda%E5%85%A5%E9%97%A82/</guid><description>本实验采用不同的方法来计算 8192 * 8192 的整型矩阵乘法运算。
C 语言版 C 语言是大家公认的高性能语言，那我们就从 C 语言开始吧。
// 用一位数组表示二维矩阵 mat1 = (int*) malloc(m_size * m_size * sizeof(int)); mat2 = (int*) malloc(m_size * m_size * sizeof(int)); result = (int*) malloc(m_size * m_size * sizeof(int)); // initialize for (int i = 0; i &amp;lt; m_size * m_size; i++) { mat1[i] = rand()/1000000; mat2[i] = rand()/1000000; result[i] = 0; } for (int r = 0; r &amp;lt; m_size; r++) { for (int c = 0; c &amp;lt; m_size; c++) { for (int n = 0; n &amp;lt; m_size; n++) { result[r*m_size + c] += mat1[r*m_size+n] * mat2[n*m_size+c]; } } } 这代码没什么可说的，值得一提的可能就是用一维数组来存二维矩阵，这样可以让矩阵在内存中的分布更连续。很容易估算出来这段代码需要进行万亿级别的乘法或加法运算，倘若使用 1950 年冯.</description></item><item><title>Cuda实战入门</title><link>https://chenminhua.github.io/posts/2019_cuda%E5%AE%9E%E6%88%98%E5%85%A5%E9%97%A8/</link><pubDate>Fri, 23 Aug 2019 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2019_cuda%E5%AE%9E%E6%88%98%E5%85%A5%E9%97%A8/</guid><description>CUDA (compute unified device architecture) 是 NVIDIA 所推出的一种并行计算平台和并行计算 api。
CUDA 在并行计算上可以大显神威，因此，我们先要找到一个可并行的问题。一个很简单的可并行问题就是计算无穷级数(infinite series)。圆周率 pi 可以通过一个著名的无穷级数(leibniz formula)进行计算，具体可查看 wiki。我们可以用此公式来逼近圆周率，c 代码如下。
c 实现莱布尼兹级数版 pi #define LOOP_N 8192000000 double pi() { double pi_qv = 1.0; int flag = -1; for (long i = 1; i &amp;lt; LOOP_N; i++) { pi_qv += flag * (1./(2 * i + 1)); flag = -flag; } return pi_qv * 4; } 下面我们编译并运行此程序，并用 time 来测量程序运行时间，在我的机器上差不多需要 8s 吧。
gcc pi_number.c -O3 -opi time .</description></item><item><title>为什么拒绝掉前37%的追求者是错的</title><link>https://chenminhua.github.io/posts/2019_%E7%99%BE%E5%88%86%E4%B9%8B%E4%B8%89%E5%8D%81%E4%B8%83/</link><pubDate>Sun, 14 Jul 2019 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2019_%E7%99%BE%E5%88%86%E4%B9%8B%E4%B8%89%E5%8D%81%E4%B8%83/</guid><description>Problem Formulation 假设你一辈子可以遇到 100 个潜在配偶，在遇到每一个潜在配偶的时候，你只能选择接受或者拒绝。如果你选择接受，则意味着你放弃了后面所有潜在的配偶；而如果你选择拒绝，则不能反悔，只能在后面的潜在配偶中选择，这也意味着如果你不小心拒绝了 the right one，你永远不能再和 TA 在一起了。现在的问题是，选择和第几个潜在追求者在一起是最佳策略？
这个问题的原型是 Secretary problem。而答案则是，一个神奇的数字–37%。数学家证明了，你应该先拒绝掉前 37%的人，然后从这开始，只要遇到一个比这前 37%的人都好的任何一个人，就选 TA。
而有一些文章说，找对象也和找秘书一样，服从这个 37%的魔数。
你可以这么来理解，假设起初你是个懵懵懂懂的男生，你对女孩儿一般都啥样一无所知，也不知道谈恋爱是干什么的，这时候你对女生的信念是无从建立的（我喜欢用信念这个词，其实就是概率分布的意思）。所以你应该干嘛呢？你应该采样，先采集一些样本，对女生建立起一种信念，这样你就知道女生一般都啥样儿，好姑娘大概啥样儿，矫情的姑娘啥样儿等等。
再说一遍，被你拒绝掉的前 37%，其实就是你的样本空间，用于刻画在你身边潜在的女生的概率分布，这帮助你在未来能客观地评价你遇到的每一个妹子。
批判 我可不是来给这种择偶方式来点赞的，而是来批判它的，从数学角度批判它。这种择偶方式存在很多漏洞，而这些漏洞都源自于其基于错误的假设。
首先，你不知道你这辈子会遇到多少潜在对象，所以你不知道哪个是第 37%个。
其次，谁说你不能反悔的，如果你找到了 80%的地方，却发现还是第一个最好，那就找第一个呗。
上面两条都不是最重要的，这种方式最大的错误是它基于一种 「所有人都满足同一个分布」的巨大错误假设。如果你是变化的，你的环境是变化的，那你遇到的潜在对象就不是同分布的，你在高中遇到的女生和你在工作中遇到的女生就不是同分布的。
你带着不同的目的会遇到不同分布的潜在对象；你在不同的年龄和状态会遇到不同分布的潜在对象；你通过不同的途径也会遇到不同分布的潜在对象。
所以比拒掉前 37% 更有用的，是找到那个更好的分布，而不是只盯着样本。这反映了人们对待统计的一个巨大的认识误区：大家往往认为是样本形成了分布，其实是分布 generate 了样本。分布才是根本，而样本只是偶然的表象。
其实，这篇文章才不是在讲怎么找对象呢。</description></item><item><title>机器学习视角下的软件工程过拟合问题</title><link>https://chenminhua.github.io/posts/2019_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98/</link><pubDate>Sun, 23 Jun 2019 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2019_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%A7%86%E8%A7%92%E4%B8%8B%E7%9A%84%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98/</guid><description>软件工程中充斥着过拟合，它不但刻画了我们在构建系统时常常掉入的思维陷阱，也刻画了我们日常生活中思考与行动的方方面面。
什么是过拟合 在机器学习中，当训练集的数据量比较少，而模型又较为复杂的时候，模型可能会出现在训练集上 fit 的非常好，但是当遇到训练集外的数据时就完全懵逼的情况，这就被称为过拟合(over fitting)。在机器学习领域，过拟合是一个非常关键的问题，甚至可以说是 No.1 的核心问题。过拟合的本质其实是训练数据量与模型复杂度之间的矛盾，由于训练数据集的大小不足以满足模型复杂度的需求，导致模型在训练集和测试集上的表现之间存在较大的差距，训练误差中的方差(variance)太高。
我喜欢将过拟合的模型比喻成一个没见过世面的年轻人，看见什么都想往自己曾经见过的、被训练和教育过的东西上套，嘴里总是在说着：”A 不就是 B 吗？”。而可能它看到的仅仅是世界的一个点、一个边、一个局部而已，当现实世界的其他测试数据向它扑来时，它就崩溃了。
软件工程中的过拟合 在软件设计的初期，我们往往并不知道完整的市场以及完整的需求，因此对软件的架构设计很可能也是有问题的。当你看着手里的这些需求分析时，你很容易开始确信自己要做的”不就是那个什么什么嘛”。这时候，过拟合很容易发生。你开始想：”我们现在有 a,b,c,d,e,f 这几种用户，看起来他们大概有这十几种需求，所以我们只要做一个软件把这十几个需求都满足了，那应该就可以满足我们所有的用户了”。在系统构建的初期，这种方式往往看起来挺美好的，你确实成功满足了当前的所有用户的所有需求。
可以过了一段时间，又来了 100 个用户，他们又提出了 300 个需求，其中有 100 个可以用现有的软件设计来满足，100 个可以通过一些扩展来搞定，而剩下的 100 个可能完全与你当初的设计背道而驰。更糟糕的是，机器学习算法可以把所有数据拿来重新训练一遍，但你的软件一旦交付了，可就不能说变就变了。在用户较少且需求分析不够完整的时候，简单地去满足所有用户的所有需求，over fitting 几乎必然发生，而这往往是致命的。
如何减轻过拟合 正则化 在机器学习算法中，正则化起到了很好的减轻过拟合的作用。事实上，我们可以将正则化看成是奥卡姆剃刀原则在机器学习中的应用。奥卡姆剃刀原理(Occam’s Razor)又称”简单有效原理”，其揭示了一个非常重要的理念：”如无必要，勿增实体”。奥卡姆剃刀原理在很多学科中都有着非常重要的应用，是一个非常底层的逻辑。而正则化其实就是利用数学方式人为的引入惩罚，惩罚过于复杂的模型，在维持模型在训练集上准确性的前提下降低模型的复杂度。当然，正则化也是有一些代价的，可能会提高一些训练误差。所以从另一个角度看可以认为正则化是训练误差换测试误差。
我们同样可以用奥卡姆剃刀原理指导我们的生活实践以及软件工程的构建，在生活中或者软件工程中应用”正则化”。无论是近年来生活方式领域流行的极简主义，或是在软件工程中保持系统的简单与可扩展性，这些都是奥卡姆剃刀原理的良好实践。
更多的数据 另一种对抗过拟合的方式是增加数据，正如前文所述，过拟合本质上揭示了训练数据量与模型复杂度之间的矛盾。理论上来说，如果你能够得到足够的的数据，就能训练出很好的模型。换一种说法，数据量决定了对模型复杂度的容忍度，数据越多，我们就越能构建一个复杂的模型。
在产品设计与软件构建中，更多的数据其实就是说，你收集的需求越多越准确，你就越能构建一个复杂而准确的系统。由此可见，大量的需求分析是多么的必不可少。那么，到底多少才算够呢？在读读上面那段话你就明白了，多少才算够取决于你要构建一个多么复杂的系统，或者说需求分析的完整性决定了对软件系统复杂度的容忍度，需求分析越完整，我们就越能构建一个复杂的系统。
预训练 有些时候，数据并不那么容易获得。幸好我们还有一种对抗过拟合的方式–预训练。预训练就是找到一个已经在某个巨大的数据集上训练过的模型，然后基于这个模型扩展出一个新的模型来。由于那个在其他大型数据集上训练好的模型已经学到了大量的相关知识，这些信息都可以为新模型所利用，因此这个新的模型就可以”站在巨人的肩膀上”。
在软件工程中，对行业已有产品或是相似产品进行准确分析定位，研究其他产品的架构设计以及方法论进行深入研究，也能够帮助我们对抗过拟合风险。</description></item><item><title>自我觉知的能力与做事的态度</title><link>https://chenminhua.github.io/posts/2019_%E8%87%AA%E6%88%91%E8%A7%89%E7%9F%A5%E7%9A%84%E8%83%BD%E5%8A%9B%E4%B8%8E%E5%81%9A%E4%BA%8B%E7%9A%84%E6%80%81%E5%BA%A6/</link><pubDate>Sun, 23 Jun 2019 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2019_%E8%87%AA%E6%88%91%E8%A7%89%E7%9F%A5%E7%9A%84%E8%83%BD%E5%8A%9B%E4%B8%8E%E5%81%9A%E4%BA%8B%E7%9A%84%E6%80%81%E5%BA%A6/</guid><description>最近对自己的行为模式进行了一些观察，发现自己很容易在遇到硬骨头的时候出现拖延症的现象。比如在进入某一主题的学习时，发现该主题内容非常丰富，且需要大量背景知识，于是当场演奏起了一段退堂鼓，或者浅尝辄止地看了些基本知识，成为一名“知道分子”。
我并不认为成为“知道分子”本身是有问题的，也不能说遇到困难打退堂鼓这个行为一定是错的。世上的事往往没有绝对的对错，而脱离上下文的评判本身才往往是错的。但是，这种遇到困难时候的个人行为模式却十分值得自我观察，以分析自己在不同情况下会有怎样的表现，为什么会有这样的表现，以及如何行动可能有更好的效果。
关于做事的态度 首先，如果有个人告诉你做事的态度应该认真负责，或者应该保守谨慎，或者其他怎么怎么样。这些建议都是不负责任的，至少是不具体的。因为首先我们要细化这个问题，即“我们在做什么事？”在面对不同的事情时，我们需要付出的努力程度显然是不一样的。那我们到底应该如何面对不同的事情呢？
有一种事情我认为你应该一次做到完美，这种事情可以称作”关闭赛道的工作”。简单解释一下，就是这个工作你做完之后，这件事情就被完美的解决了，以后遇到这个问题，你都不需要再解决一遍，甚至你的朋友，或者其他任何人遇到这个问题，他们都不需要再从头解决一遍，只需要参考你的工作就行了。比如我在上一篇关于我组装一台 PC 的文章中，就引用了 Tim Dettmers 的关于如何选择深度学习 GPU 的文章，事实上，当我在调研这个问题的时候，我查到的大量关于此问题的讨论中，很多人都引用了他的这篇文章，以至于我认为，所有在对这个问题进行的讨论中，如果没有提到这篇文章，那么这次讨论的背景信息就是不完整的。
还有一类事情则是”永续型工作”，这类工作没有终点，也就是赛道永远不会关闭。比如跑步，健身，学习，科研。在面对这类事情的时候，我们需要更多的耐心，而不是一时的冲动和激情。如果在每一次跑步时，都追求完美，可能反而会消磨你的意志，让你无法坚持下去。</description></item><item><title>Got a PC</title><link>https://chenminhua.github.io/posts/2019_got_a_pc/</link><pubDate>Tue, 18 Jun 2019 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2019_got_a_pc/</guid><description>从 15 年初购入人生第一台 mbp 之后，再也没有使用过 PC 了。最近趁着 618 组了一台电脑，平时的主力机器是 mbp，然后家里放了台 mac mini，但是最近为了在家里跑一些深度学习的模型，而 mac 的显卡实在是让人一言难尽。另外，由于苹果最近和英伟达开始了”战争”，而英伟达又是搞深度学习绕不过去的，万般无奈，只好自行装了台机器。
可以说，我是为了张显卡装了台机器。。。开始的预算是 8000 块，最后大概超出预算 30 块样子吧。下面简单记录一下装机组件列表与一些注意点。
组件 CPU+主板 cpu 和主板一定要配套买，最好不要分开买。另外 cpu 和主板要先选好，要注意看主板的接口。我买的是 i7 8700 + Z370 套装。其中 i7 8700 cpu 是 6 核，3.2G 主频，12 线程。3370 主板支持 4 个 DDR4 插槽，3 个显卡槽。
注意，i7 8700 可以使用自带的 cpu 风扇，如果买其他型号的 cpu 可能需要升级一下 cpu 风扇。
机箱 机箱不能太小，第一是装机的时候方便，第二是考虑以后的可扩展性。除此之外，机箱是否静音也比较重要。我买的是先马黑洞，中塔机箱，内置 3 个风扇。
电源 一定要 600W 以上的。电源大小要看好，确保能放入机箱内，如果机箱比较大的话，一般都没什么问题。我买的是 silverstone 的 ET650-G,额定功率 650W。
显卡 毕竟是为了显卡才装的机器，显卡是关键。参考 Tim Dettmers 的这篇文章，如果没有时间的话可以看他最后写的 TL,DR。如果还没有时间并且有 3200 块预算的话，先买个 RTX2070 吧。我买的是索泰 RTX2070，8G 显存，2304 个 CUDA 核心。京东价格在 3200 到 3400 间吧。</description></item><item><title>当我们谈论杜兰特，我们其实是在谈自己</title><link>https://chenminhua.github.io/posts/2019_%E5%BD%93%E6%88%91%E4%BB%AC%E8%B0%88%E8%AE%BA%E6%9D%9C%E5%85%B0%E7%89%B9/</link><pubDate>Thu, 13 Jun 2019 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2019_%E5%BD%93%E6%88%91%E4%BB%AC%E8%B0%88%E8%AE%BA%E6%9D%9C%E5%85%B0%E7%89%B9/</guid><description>人们谈论问题容易陷入对意义的讨论，这种试图对本质的窥探虽然“有用”，却也十分危险。
早上看到杜兰特跟腱受伤的新闻，各大媒体纷纷表示，杜兰特在总决赛第五场不应该复出，并指责勇士队在第五场让杜兰特上场的决定（从结果上看，这确实是一个错误的决定）。比较突出的表述是：“都已经 1 比 3 了，为了拿个冠军，冒着职业生涯报废的危险上场，有啥意义？杜兰特被道德绑架，杜兰特被勇士队利用了，勇士对不起杜兰特…”。
事实上，我们在说的并不是杜兰特和勇士，而是在说我们自己。我们在表达的是：“如果我是杜兰特，我才不会冒着这种风险复出。如果我是杜兰特，我现在肯定恨死勇士了。”事实上，我们谁又不是杜兰特呢？我们从小受到的个人英雄主义教育与集体主义教育都将可能驱使我们做出和杜兰特一样的决定。比如在关键项目 deadline 到来的时候，冒着猝死的风险加班写代码，杜兰特为的是勇士能够赢下总冠军以及自己的个人荣誉，而你为的是团队项目的成功以及老板的赏识以及年终奖。
除了利益的驱使外，更大的推动力来自于“不做的惩罚”。如果杜兰特决定第五场不上场，勇士可能就会直接输掉总决赛，而他也就成了那个坐在板凳上看队友被按在地上的男人；勇士也可能会绝地反击赢得冠军，而杜兰特就成了那个可有可无的人。这样的惩罚实在太大了，比不加班就没有年终奖大多了。 我又想起了姚明和王治郅。姚明在 nba 打球的时候，几乎每年夏天都要带伤为国家队打比赛，或许这是姚明职业生涯短暂的原因之一。而王治郅则曾经因为不愿打国家队的比赛而被开除，并受到国内舆论媒体的指责。那时候的我们是怎么说的呢？我们说：“姚明是好样的，男人就应该这样。王治郅是个叛徒”。但现在，如果易建联说自己有伤夏天不能打国家队的比赛，指责的声音应该不会太多了吧。
这里面似乎存在一种从集体主义到个人主义的转变。在这一过程中，个人利益与价值得到了更多的尊重，可人却活的越来越虚无。如果冒着职业生涯报废的危险上场比赛没有意义，那冒着生病乃至猝死的危险加班又有什么意义？结婚或者生二胎又有什么意义？孝顺父母又有什么意义？甚至于一些个人行为比如健身读书写作思考都又有什么意义？这种看似是对人生意义的思考，轻而易举地将我们带入了虚无主义。更糟糕的是，这种思考在对个人行为进行批判的同时，并没有告诉我们应该怎么做。
杜兰特复出的决定看起来是错的，但是从结果反推行为没有多大意义，人生不过就是：做出选择，承担后果。我为杜兰特的决定骄傲，也希望他能早日康复。</description></item><item><title>语言的雅俗</title><link>https://chenminhua.github.io/posts/2019_%E8%AF%AD%E8%A8%80%E7%9A%84%E9%9B%85%E4%BF%97/</link><pubDate>Tue, 04 Jun 2019 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2019_%E8%AF%AD%E8%A8%80%E7%9A%84%E9%9B%85%E4%BF%97/</guid><description>昨天看到 slack 里面有个网友说：”这是我下午看《洞察》的时候突然冒出来一个问题：钱有许多称呼，比如阿堵物、孔方兄等，可是既然它们指的都是相同的东西，为什么又有雅俗的分别？钱可能还好，现在应该没有太多人会觉得追求钱是羞耻的，更好的例子比如生殖器、月经等。”
这个问题我认为或许和场合有关。
从自然语言处理视角来看，这里的“场合”可以建模为一个词在作为 center word 时，其周围的 context word 的概率分布的区别。比如具有相同指向物的两个描述生殖器的词汇，其中一个的 context word 多为医学词汇，而另一个则可能多为侮辱性词汇。相应的，这两个词在统计意义上更接近于不同的主题模型，因而给人雅和俗的不同感受。</description></item><item><title>什么是概率</title><link>https://chenminhua.github.io/posts/2019_%E4%BB%80%E4%B9%88%E6%98%AF%E6%A6%82%E7%8E%87/</link><pubDate>Sun, 12 May 2019 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2019_%E4%BB%80%E4%B9%88%E6%98%AF%E6%A6%82%E7%8E%87/</guid><description>假设我们现在要扔一枚硬币，我问你这枚硬币正面朝上的概率是多少？你一定知道是 0.5。但是让我们来仔细想想，概率为 0.5 到底意味着什么呢？一种常见的解释是，如果我们扔足够多次的硬币，会有大概一半的情况为硬币正面朝上。
可是，如果这是一枚只能扔一次的硬币呢？事实上这个世界上充满了只能扔一次的硬币。每年的欧冠决赛只能踢一场，那么这时候我们又如何来理解”利物浦有 55%的几率会夺得欧冠冠军”这句话呢？你总不能让热刺和利物浦在一百万个平行宇宙里面踢上一百万场欧冠决赛，然后发现利物浦赢了其中的 550000 场吧。
那当我们说”利物浦有 55%的几率夺得欧冠冠军”的时候，我们到底在说什么？
回到扔硬币的问题上来，若我们用宿命论的观点来看这枚硬币，那么它在被抛出前，如何落地应当就已经决定了，在宇宙大爆炸的那一刻就被决定了。每一个电子的运动，都只是沿着它宿命的轨迹罢了。但是在这枚硬币落地之前，我们依然不知道它会如何落地，正面朝上和反面朝上的概率都是 0.5。现在，我偷偷告诉你一些事情，我告诉你这枚硬币其实不是一枚正常的硬币，它被特殊处理过，使得它的正面更容易朝上（比如说扔 100 次有 70 次都是正面朝上）。现在，在硬币落地之前，你就知道硬币有 70% 的概率正面朝上了。
这还不够，我现在想告诉你更多的东西，我想告诉你扔出硬币的时候硬币的受力分析，以及当时的重力加速度，硬币距离地面的高度，硬币的重量和质心，硬币形状，硬币弹性，地面的弹性。这时候你可以对这个”扔硬币系统”进行力学模拟了。当然，由于你对地面摩擦系数，空气阻力，大气压强等信息并不确定，所以你需要在这些参数上引入一些随机性，有了前面加入的这些先验知识后，经过一系列经典力学的计算，你对于硬币正面朝上这件事开始变得更有信心了（也可能更没有信心）。
更进一步，我现在把地面摩擦系数，空气阻力，大气压强等统统告诉你，我把所有可能影响硬币落地状态的所有知识都告诉你，现在，在硬币落地前，你就已经可以算出硬币是正面朝上落地还是反面朝上落地了，正面朝上的概率变成了 1（或者 0）。对于你而言，这件事已经不再是一个随机事件了（但是对于其他人来说，他们不知道这一切，所以在他们看来，正面朝上的概率依旧是 0.5）。
原来，概率可以是一件这么主观的事情，它并不仅仅用于描述一个事件本身的某些特质，它还和人对于事件的观察有关，它关乎于一种信念，关乎于你对事件既有的认识。庞加莱说：概率只是对我们无知程度的度量。
总结一下，上面的两种观点分别称为频率学派和贝叶斯学派。频率学派认为概率反映的是系统的一些特质，由系统的本质决定的；而贝叶斯学派则认为概率关乎于信念。
事实上，我认为这两种观点并不相互矛盾，有时候，他们只是在阐述不同的问题而已。现在我们再回来看看”利物浦有 55%的几率夺的欧冠”这句话，频率学派可能会觉得 55% 是由事件本质属性决定的，是伟大的自然法则控制的一个数字。但在我看来，我并不认同这种看法，我认为 55% 并非系统本质的完全描述，而只是某个人或者某些人的观点罢了（即使他们认为他们的观点来自于对自然法则的精确分析，这也只代表他们的观点）。事实上，不同的人对于这场比赛可能会有完全不同的预测，比如热刺主教练，利物浦球员，博彩公司的高级分析师，普通的英超球迷，压根不看球的人…每个人对于谁会夺冠都会有着完全不同的观点。</description></item><item><title>生活中的弹力系统设计</title><link>https://chenminhua.github.io/posts/2019_%E7%94%9F%E6%B4%BB%E4%B8%AD%E7%9A%84%E5%BC%B9%E5%8A%9B%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/</link><pubDate>Sat, 11 May 2019 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2019_%E7%94%9F%E6%B4%BB%E4%B8%AD%E7%9A%84%E5%BC%B9%E5%8A%9B%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/</guid><description>艺术来源于生活，系统架构也是如此。
基于微服务的分布式系统架构所面临的挑战非常巨大，充满了各种不确定性。为了能够提高 SLA，我们需要让系统能够更有弹性，在系统部分出现故障的情况下，尽可能地减少损失。常见的弹性系统设计模式有：降级，限流，重试，补偿，异步，幂等，隔离，熔断等等。本文将结合生活中的一些例子来介绍这些模式是什么，以及什么时候应当考虑使用这些模式。
降级模式 所谓降级模式就是，当服务出现资源瓶颈，吞吐量跟不上的时候（注意是吞吐量），为了让系统能够正常运行，并承受常规情况下吞吐上限，牺牲掉一些相对次要的功能，保住关键业务的一种设计模式。比如在电商秒杀活动中，如果系统流量过大吞吐跟不上，就可以考虑牺牲掉一些 feature，比如用户评价啊，商品具体的详情啊等等，都不进行展示（都已经是秒杀了，谁还看那些东西）。
在生活中，当高速公路收费站排队太长的时候，路网系统会手动切换到免费放行模式。在节假日这种明显会产生大流量的情况下，系统还会自动切换到免费放行模式。其实这就是一种降级设计，通过牺牲收费这个相对次要的功能，保住了车辆通行顺畅这一关键业务。
限流模式 限流设计对关键业务的保护尤其重要，比如说用户中心就是一个关键业务，是一个不能挂的服务，而如果说现在有个程序员写了一个 bug，在某种 corner case 下会导致某个服务不断地去用户中心拉取大量数据，这种时候就很容易导致用户中心的请求队列里面堆积太多请求，原本正常的请求反而会被 delay，甚至得不到正确响应。
另一种常见的使用限流模式的场景出现在 open api 的设计中，由于你的 api 不再是内部系统调用的，而是暴露给第三方，你根本不知道别人会怎么用你的 api 啊，这时候限流就成为你必须要考虑的事情。
还是举一个生活中道路交通的例子，每次放假结束回上海的时候，都会遇到交警在高速公路崇明路段提前收窄道路的情况，人为降低道路通行能力。这在某种程度上就是为了缓解上海长江隧道的拥堵情况，让大家不要都堵在一个点，而是选择绕行，或者在服务区休息休息，或者看到道路拥堵就改个时间出行。。
重试模式 重试模式与 CAP 有关。分布式事务也是系统设计中的大坑，而在微服务的语境下，这个大坑往往很难避免。如果你知道 CAP 理论，你应该明白我们总是要在 C 和 A 之间做出一些牺牲。对于那些对一致性要求极高的系统（比如银行转账），有时候我们只能选择牺牲一定的可用性，但是对于更多的系统来说，往往我们可以考虑争取更高的可用性，而牺牲掉一些强一致性。重试模式就是牺牲强一致性而追求更高可用性的一种设计模式。
假设某个事务需要改变 A，B 两个系统的状态，但是当 A 的状态成功发生改变后，B 系统却迟迟不能响应，或者 B 系统干脆挂掉了。这时候如果对强一致性没有那么高的要求，你可以选择稍微等待片刻后，重新对系统 B 发起请求（B 系统的对应接口应当是幂等的）。如果你运气不错，可能重试个一两次，B 系统就活过来了，事务也就能成功完成了。
这很像我们在网上买东西，快递员给你送过来，结果你说你今天出差不在家，让他明天再来。于是快递员明天又来了一回，你拿到了你买的东西，交易事务也就成功了。
补偿模式 补偿模式则是处理牺牲强一致性而追求更高可用性的另一种设计模式，通常会和重试模式配合使用。还在上面的例子，A 系统状态更新成功了，但是在请求 B 系统的时候，B 系统却报了个错，表示这个交易无法完成（对方账户没了啊，商品库存不够了啊，商品已经下架了啊）。这时候无论你怎么进行重试，事务都不可能完成了。那怎么办呢？你需要去补偿 A 系统。你要告诉 A 系统：”有内鬼，交易终止”，然后让 A 系统补偿之前的状态更新操作（账号上把钱加回来啊啥的）。
当然，你一定会想到，如果补偿也失败了呢？而且是那种业务上完全无法完成的补偿，这时候咋办？理论上来说，我觉得这个问题是无解的。比如 A 让你把钱交给 B，但是在你给到 B 之前 B 被人杀了，你想把钱还给 A，结果发现 A 也被杀了，那么这件把 A 的钱转交给 B 的事务就用于没法被”做完”，或者被”没做”。对于这种问题，我们只能说减小它发生的概率（比如设计一些两阶段提交之类的东西，当然这显然增加了系统的复杂度，而我们讨厌复杂度），并且设计好兜底方案。如果说你的业务真的无法接受这件事情的发生的话，恐怕你只能让 A 自己直接把钱交给 B 了（不要拆分这两个系统）。</description></item><item><title>信息化与自动化</title><link>https://chenminhua.github.io/posts/2019_%E4%BF%A1%E6%81%AF%E5%8C%96%E4%B8%8E%E8%87%AA%E5%8A%A8%E5%8C%96/</link><pubDate>Sun, 10 Mar 2019 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2019_%E4%BF%A1%E6%81%AF%E5%8C%96%E4%B8%8E%E8%87%AA%E5%8A%A8%E5%8C%96/</guid><description>这些年来，”工业 4.0”，”2025 计划”等新名词越来越多的出现在人们的视野，工业互联网概念得到了各国政府部门以及产业界的重点关注。尽管对于什么是”工业 4.0”依旧还没有一个固定的标准，但是在这条向未来探索的路上，人们渐渐摸出了一些方向。未来的工业，可能会呈现出这样一番景象：所有的生产资源（人，机，料）都通过网络被连接起来并相互感知，并通过智能系统的控制完成生产决策，生产过程中的所有信息都将可被追溯，而这将全面改变现有的生产管理与供应链管理。未来或许已经并不遥远，而在未来到来之前，工业界更多地在做的是这两件事：信息化和自动化。
信息化和自动化都并非新鲜事，可以说从第一次工业革命开始，工业就已经走上了信息化与工业化的道路。200 年前，人们就已经开始用蒸汽机取代人类完成一些生产工作。我曾经参观过青岛啤酒的产线，一个巨大的车间里面，只有数十个工作人员，从一袋袋小麦到一箱箱啤酒，工作人员只需要在现场“监督”那些巨大的机器工作，并进行一些质检工作，而在现场的屏幕上，实时地刷新着生产的具体情况：正在进行哪个工单的生产，进度如何等等。
当然，在现有的工业体系中，能够很好地做到信息化和自动化的企业并不多，提前上车的企业往往能够率先得到信息化与自动化带来的红利，而要真正提高整个行业的效率，则必须在全行业推行先进的生产技术与管理模式。
信息化往往更偏向与管理上的升级，而自动化更偏向于技术上的升级。事实上，不止是工业界，很多领域往往都可以一分为二的从管理和技术两个方面上分析。
举个例子，为什么 google cloud 落到了 aws 和 azure 的后面？google 拥有世界上最好的搜索引擎，也就意味着它有着最好的流量，而云服务又是面向互联网企业的服务，google 完全可以凭借其搜索引擎巨大的流量优势来吸引互联网企业，但是它却掉到了亚马逊和微软的后面，到底是啥原因呢？原因就在于 google 骨子里就是一家技术上成功的公司，他们擅长的打法就是做出一个世界上最牛逼的产品，然后你们自己来用。而云服务某种程度上是服务业，你需要更好地去服务你的客户，帮助你的客户获得成功，而这种模式不是仅仅有技术就能做到的，更需要先进的管理来支撑。
对于工业界来说，技术上的成功与管理上的成功都非常重要，信息化与自动化也需要齐头并进。
小结 随着硬件计算能力的增强，网络基础设施的普及，人工智能与大数据分析技术的发展，信息化和自动化还将向前大跨一步。信息化与自动化也将更加的融为一体。从供应链管理，生产资料管理，生产过程管理等环节，都将被准确地记录并实时地反馈出来，而这些关键的信息也将对生产的自动化产生有价值的指导。另一方面，自动化带来的大量数据也将被信息化记录，并可从中分析出更多精细的数据。</description></item><item><title>关于麻将能不能进入奥运会的讨论</title><link>https://chenminhua.github.io/posts/2019_%E5%85%B3%E4%BA%8E%E9%BA%BB%E5%B0%86%E8%83%BD%E4%B8%8D%E8%83%BD%E8%BF%9B%E5%85%A5%E5%A5%A5%E8%BF%90%E4%BC%9A%E7%9A%84%E8%AE%A8%E8%AE%BA/</link><pubDate>Sun, 10 Mar 2019 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2019_%E5%85%B3%E4%BA%8E%E9%BA%BB%E5%B0%86%E8%83%BD%E4%B8%8D%E8%83%BD%E8%BF%9B%E5%85%A5%E5%A5%A5%E8%BF%90%E4%BC%9A%E7%9A%84%E8%AE%A8%E8%AE%BA/</guid><description>这篇文章来源于一群软件工程师在晚餐时的讨论，论题是中国麻将能不能进入奥运会。这类问题在不同的群体间相信已经发生过无数次，这次当然也毫无例外的并没有得出任何有价值的结论。但这并不妨碍这是一个有意思的问题，并值得花一些时间思考。
先说说我的观点吧，我个人并不看好麻将进入奥运会。在我看来，麻将是一个比较容易出现弱胜强情况的游戏（相信大家对这一点都有所体会，一个第一次玩麻将的人居然连续赢了好几把钱）。
说到这，我们遇到了一个比较关键的问题，如何定义强和弱？
如何量化一个麻将手的实力？很容易想到大数定理。根据大数定理我们知道，只要实验次数趋近无穷，实验结果的均值就会趋近于事物的本质。也就是说，如果麻将高手和麻将菜鸟打上无数圈麻将，赢的多的那个就更强。在不考虑打麻将风格相克之类的因素的假设下，我们甚至可以设立一个基准麻将手，并用一个选手和基准麻将手玩的成绩来量化一个麻将手的能力。
问题是，大数定理只有在实验次数非常非常多的情况下才奏效，但是在奥运会上，我们甚至不可能打一百圈麻将。而至于打多少圈以上我们才能相信战绩体现了实力，这又是另一个有趣而值得思考的问题了。
当然，我的观点也遭到了一些反驳。理由是：竞技体育中也充满了运气因素，即使 nba 总决赛也不过只是打七场而已（事实上这取决于你如何看待这个问题中的事件，究竟比赛是一场一场打的，还是一个回合一个回合打的，这又是一个有趣且非常复杂的问题）。
在我看来，这种观点存在一些缺陷，或者说这种类比并不准确。麻将正在的问题在于，在所有玩家 take action 前（摸完牌，但是打出第一张牌前），在玩家实力未知的情况下，每个玩家获胜的条件概率(given 发牌情况与后面码好的牌的情况)是不均匀的，并且可能是严重不均匀。简单点说，就是有的玩家一手好牌，有的则一手烂牌。竞技体育则不同，在球队实力未知的情况下，在开球前两支球队获胜的概率几乎是相同的。
对于麻将这类游戏，我有另外一种类比。假设我们对奥运会男子 400 米赛的规则进行一些修改，增加一些人为的趣味性（不确定性），在发令枪响前，由一个随机系统给每个运动员分配一个 1 到 8 的数字（不重复，记为 i），每个选手需要跑完 400 + 0.1i 米的距离，或许对于博尔特之类的选手，即使抽到 8 也有机会夺冠。而如果我们修改一下规则，每个运动员要在比赛中跑完 400 + 10i 的距离，也就是说抽到 1 的要跑 410 米，而抽到 8 的要跑 480 米。这时候场外因素显然开始对比赛产生影响了，抽到 1 的选手即使实力最差，也极有可能第一个完成比赛获得冠军。如果我们再修改一下规则，每个运动员要在比赛中完成 400 + 1000*i 的距离，那这场比赛几乎就成了一场抽签比赛，因为只有有选手抽到了 1，他几乎就稳赢了，这个时候每个选手的胜率几乎是一样的。当然，我们依然可以说这是一场公平的比赛，但是这不是一个好的竞技项目。
扔硬币也是公平的比赛。假设现在奥运会加入了一场扔硬币比赛，我们几乎可以说，在玩家开始扔之前，玩家们的获胜概率必然是均匀的。倘若真的能够练出在非常高的扔均匀硬币次数下让硬币正面朝上的次数明显高于反面朝上次数的能力，那么在概率统计的意义上来说，扔硬币就是一种比麻将更适合于竞技的游戏。
事实上，我们只需要再修改一下规则，这个田径比赛就会更接近麻将游戏。假设每次发令枪响前，每个选手会抽中一个实数 i,然后他需要跑 400 + i 的距离，其中 i 服从正态分布。这个模型还存在一点点缺陷，因为麻将摸牌不是独立事件，所以在把 i 变成一个向量，服从高维正态分布。多么简洁而漂亮的模型！
稍微总结一下，什么样的游戏适合竞技呢？我觉得需要满足两点。第一，在玩家能力未知的情况下，玩家获胜的概率是近似于均匀分布的。第二，在玩家能力已知的情况下，玩家获胜的概率应当偏离均匀分布（学过信息论的同学应该一下就能想到信息增益的概念吧）。通俗点说就是：外部影响因素要少，选手能力影响要大（至少要有影响吧，不然真成抛硬币了）。</description></item><item><title>模型设计的两大准则：NFL与奥卡姆剃刀</title><link>https://chenminhua.github.io/posts/2019_nfl%E4%B8%8E%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80/</link><pubDate>Sat, 09 Mar 2019 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2019_nfl%E4%B8%8E%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80/</guid><description>NFL(无免费午餐)定理 模型的合理性很大程度上取决于待解决问题本身的特征。
无免费午餐（No Free Lunch, NFL）定理证明了任何模型在所有问题上的性能都是相同的，其总误差和模型本身是没有关系的。可是既然大家谁都不比谁好，那关于机器学习算法和模型不计其数的研究又有什么意义呢？理解 NFL 的关键在于“所有问题”的表述。
从模型的角度来看，如果单独拿出一个特定的模型来观察的话，这个模型必然会在解决某些问题时误差较小，而在解决另一些问题时误差较大。从问题的角度来看，如果单独拿出一个特定的问题来观察的话，必然有某些模型在解决这些问题时具有较高的精度，而另一些模型的精度就没那么理想了。
NFL 定理最重要的指导意义在于先验知识的使用，也就是具体问题具体分析。说白了就是：没有最好的模型，只有最合适的模型
奥卡姆剃刀原则 如果有多种模型都能够同等程度地符合同一个问题的观测结果，那就应该选择其中使用假设最少的，也就是最简单的模型。一个问题存在多个可接受的模型，其中的每一个都可以演化出无数个更为复杂的变体，其原因在于可以把任何解释中的错误归结于某种特例的出现，将这个特例纳入模型就可以避免原来错误的发生。更多特例的引入无疑会降低模型的通用性和可解释性，把薄薄的教材变成厚重的词典，这就是奥卡姆剃刀偏爱简单模型的原因。
过于简单的模型就像做快餐，按照一定的参数标准统一的做几个菜，可能老板和服务员自己都觉得一般般，广大消费者吃起来自然也不会觉得多好吃。就像欠拟合的模型在训练集上都没有良好的表现，别提泛化了。相比之下，过于复杂的模型则是私人定制餐点，你爱吃酸的我就放酸的，可是真的拿到市场上卖，爱吃酸的人并不多，这就像过拟合的模型能够在训练集上表现优异，却不具备良好的泛化性能。
过拟合也好，欠拟合也罢，都是想避免却又无法避免的问题。用较为简单的模型来模拟复杂的数据生成机制，欠拟合的发生其实是不可避免的。可欠拟合本身还不是更糟糕的，更糟糕的是模型虽然没有找到真正的相关性，却自己脑补出一组关系，并把自己的错误的想象当做真实情况加以推广和应用，得到和事实大相径庭的结果——其实就是过拟合。
模型的复杂度也可以从误差组成的角度一窥端倪。模型的误差包括三个部分：偏差（bias），方差（variance）和噪声（noise）。噪声是不可约误差（irreducible error），并不能通过模型的训练加以改善。除了噪声之外，偏差和方差都与模型本身有关，两者对误差的影响可以用误差的偏差 - 方差分解（bias-variance decomposition）来表示。偏差的含义是模型预测值的期望和真实结果之间的区别，如果偏差为 0，模型给出的估计的就是无偏估计。但这个概念是统计意义上的概念，它并不意味着每个预测值都与真实值吻合。方差的含义则是模型预测值的方差，也就是预测值本身的波动程度，方差越小意味着模型越有效。抛开噪声不论，模型的误差就等于偏差的平方与方差之和。
偏差和方差之间的折中与模型自身的特性息息相关。偏差来源于模型中的错误假设，偏差过高就意味着模型所代表的特征和分类结果之间的关系是错误的，对应着欠拟合现象；方差则来源于模型对训练数据波动的过度敏感，方差过高意味着模型对数据中的随机噪声也进行了建模，将本不属于特征 - 分类关系中的随机特性也纳入到模型之中，对应着过拟合现象。
根据上面的理解，就不难得到结论：理想的模型应该是低偏差低方差的双低模型，就像一个神箭手每次都能将箭射进代表 10 环的红心之内；应该避免的模型则是高偏差高方差的双高模型，这样的箭手能射得箭靶上到处窟窿，却没有一个哪怕落在最外层的圆圈里。更加实际的情形是偏差和方差既不会同时较低，也不会同时较高，而是在跷跷板的两端此起彼伏，一个升高另一个就降低。
一般说来，模型的复杂度越低，其偏差也就越高；模型的复杂度越高，其方差也就越高。比较简单的模型像是个斜眼的箭手，射出的箭都在远离靶心的 7 环的某一点附近；比较复杂的模型则是个心理不稳定的箭手，本来是 9 环水平却一下射出 10 环一下射出 8 环。对模型复杂度的调整就是在偏差 - 方差的折中中找到最优解，使得两者之和所表示的总误差达到最小值。这样的模型既能提取出特征和分类结果之间的关系，又不至于放大噪声和干扰的影响。</description></item><item><title>身高背后的统计学</title><link>https://chenminhua.github.io/posts/2019_%E8%BA%AB%E9%AB%98%E8%83%8C%E5%90%8E%E7%9A%84%E7%BB%9F%E8%AE%A1%E5%AD%A6/</link><pubDate>Fri, 08 Mar 2019 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2019_%E8%BA%AB%E9%AB%98%E8%83%8C%E5%90%8E%E7%9A%84%E7%BB%9F%E8%AE%A1%E5%AD%A6/</guid><description>给同年龄的一百个小孩量身高，已经量了 99 个，请问最后一个小孩的身高有 99%的概率会小于多少。
我们只知道这些数据来自同一个总体（服从同一个分布），前 99 个小孩和最后一个是一视同仁的，也就是说，这一百个小孩每个都有 1%的可能是最高的。所以最后那个小孩的身高有 99%的概率小于前面 99 个小孩中最高的那个。
而一些同学可能会有不同的观点，通常来说同龄小孩身高会服从正态分布。通过分析前 99 个小孩的身高我们是可以算出这个正态分布的参数的。假设均值为 100，方差为 4，则可以很容易算出最后一个小孩以 99%的概率低于 100 + 2 * 2.365 = 104.73 厘米。
参数模型与非参数模型 （parametric/non-parametric model） 上面两种方法中，第一种是非参数模型，它并不关心这些数据属于什么分布。第二种则是参数模型，其依赖于一个很强的先验假设：身高服从正态分布。换句话说，如果模型有一组固定的参数来描述其概率分布，则为参数模型；否则为非参数模型。
参数模型的优点是只有几个少量的参数就刻画了整个模型，每个参数都有其明确的统计意义（比如上面的均值和方差），但是它需要依赖于超强的先验假设：所有数据符合特定类型的概率分布。学习的过程就是利用训练数据估计未知参数的过程，这些估计出来的参数就是训练数据的浓缩。在这个过程中，先验知识确定了假设空间的取值范围，学习算法（比如最大似然估计或是最大后验概率估计）则在给定的范围内求解最优化问题。
可如果先验分布本身就不符合实际（换句话说你的假设空间错了），学习算法再强，数据再好也得不出正确结果。所以当对所要学习的问题知之甚少的时候，应当避免对潜在模型做出过多的假设。这类不使用先验信息，完全依赖数据进行学习得到的模型就是非参数模型。
非参数模型不是“无参数模型”，恰恰相反，非参数模型意味着模型参数的数目是不固定的，并且极有可能是无穷大，这决定了非参数模型不可能像参数模型那样用固定且有限数目的参数来完全刻画。在非参数模型中不存在关于数据潜在模式和结构化特性的任何假设，数据的所有统计特性都来源于数据本身，一切都是“所见即所得”。当训练数据趋于无穷多时，非参数模型可以逼近任意复杂的真实模型，而代价是非常大的时空复杂度。
归根结底，非参数模型其实可以理解为一种局部模型，就像战国时代每个诸侯国都有自己的国君一样，每个局部都有支配特性的参数。在局部上，相似的输入会得到相似的输出，而全局的分布就是所有局部分布的叠加。相比之下，参数模型具有全局的特性，所有数据都满足统一的全局分布，这就像履至尊而制六合得到的扁平化结构，一组全局分布的参数支配着所有的数据。
数据模型(data model)与算法模型(algorithm model) 从数据分布的角度看，不同的模型可以划分为参数模型和非参数模型两类。如果将这个划分标准套用到模型构造上的话，得到的结果就是数据模型（data model）和算法模型（algorithm model）。相比于参数对数据分布的刻画，这种分类方式更加侧重于模型对数据的拟合能力和预测能力。
http://www2.math.uu.se/~thulin/mm/breiman.pdf 统计模型，两种不同的文化。
作为一个统计学家，布雷曼看重的是学习算法从数据中获取有用结论和展示数据规律的能力。从这一点出发，他将从输入 xx 到输出 yy 的关系看成黑盒，数据模型认为这个黑盒里装着一组未知的参数，学习的对象是这组参数；算法模型则认为这个黑盒里装着一个未知的映射 f()˙f()˙，学习的对象也是这个映射。
数据模型和参数模型类似，其最典型的方法就是线性回归。线性回归的含义明确而清晰：输入数据每个单位的变化对输出都会产生同步的影响，影响的程度取决于这个特征的权重系数，不同特征对结果的贡献一目了然。
算法模型和非参数模型类似，其著名代表就是随机森林算法。随机森林是一种集成学习方法，构成这座森林的每一颗树都是决策树，每一棵决策树都用随机选取数据和待选特征构造出来，再按照少数服从多数的原则从所有决策树的结果中得到最终输出。
如果说参数模型与非参数模型的核心区别在于数据分布特征的整体性与局部性，那么数据模型和算法模型之间的矛盾就是模型的可解释性与精确性的矛盾。数据模型有更好的可解释性，而算法模型则相对而言抛弃了可解释性（对于特别复杂的问题，追求可解释性几乎是不可能的，追求精确性更重要）。
决策树本身是具有较好可解释性的数据模型，它表示的是几何意义上对特征空间的划分，但是精确度却不甚理想。随机森林解决了这个问题：通过综合使用建立在同一个数据集上的不同决策树达到出人意料的良好效果，在很多问题上都将精确度提升了数倍。但精确度的提升换来的是可解释性的下降。每个决策树对特征空间的单独划分共同织成一张剪不断理还乱的巨网，想要理解这张巨网背后的语义无异于水中望月、雾里看花。
生成模型与判别模型 还有另一种针对学习对象的划分方式，那就是生成模型和判别模型之分。简单地说，生成模型学习的对象是输入 xx 和输出 yy 的联合分布 p(x,y)p(x,y)，判别模型学习的则是已知输入 xx 的条件下，输出 yy 的条件分布 p(y|x)p(y|x)。两个分布可以通过贝叶斯定理建立联系。
生成模型和判别模型的区别可以这样来理解：假如我被分配了一个任务，要判断一个陌生人说的是什么语言。如果用生成模型来解决的话，我就需要把这个老外可能说的所有语言都学会，再根据他的话来判定语言的种类。但可能等我学完这些语言时，这个陌生人都说不出话了。可是用判别模型就简单多了，我只需要掌握不同语言的区别就足够了。即使不会西班牙语或者德语的任何一个单词，单凭语感也可以区分出这两种语言，这就是判别模型的优势。
针对生成模型和判别模型的利弊，支持向量机的奠基者弗拉基米尔·瓦普尼克（Vladimir Vapnik）有句名言：“（解决分类问题）应该直截了当，不要用兜圈子的方式，搞一个更难的问题（比如求解似然概率）做为中间步骤”。一般来说，生成模型的求解更加复杂，当数据量趋于无穷大时，渐进条件下的精确性也更差，但其收敛的速度更快，在较少数据的训练后就可以收敛到错误的下界。相比之下，判别模型的形式更加简单，在分类问题上的表现也更出色，却不能提供关于数据生成机制的信息。有些情况下，生成模型和判别模型会成对出现。例如在分类问题中，朴素贝叶斯和逻辑回归就是一对生成 - 判别分类器。</description></item><item><title>一场赌局背后的统计学</title><link>https://chenminhua.github.io/posts/2019_%E4%B8%80%E5%9C%BA%E8%B5%8C%E5%B1%80%E8%83%8C%E5%90%8E%E7%9A%84%E7%BB%9F%E8%AE%A1%E5%AD%A6/</link><pubDate>Thu, 07 Mar 2019 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2019_%E4%B8%80%E5%9C%BA%E8%B5%8C%E5%B1%80%E8%83%8C%E5%90%8E%E7%9A%84%E7%BB%9F%E8%AE%A1%E5%AD%A6/</guid><description>A 和 B 两人进行一场赌局，方式是抛一枚特制的硬币。如果硬币正面朝上，则 A 得一分，反之 B 得一分，先得六分的获胜。假设现在 A 以 5：3 领先，请问最终 B 获胜的概率要多高。
概率学派 频率学派认定待估计的参数是固定不变的常量，这里也就是硬币正面朝上的概率，用 p 来表示。频率学派认为，A 之所以 5：3 领先，是因为 5：3 这种情况出现的概率高，所以 p 应该是能够让 5：3 出现的概率最高的值。而求这个值的方法通常是最大似然法。此处似然函数为 L=p^5(1-p)^3,令一阶导=0 得出 p=5/8。而 B 获胜的概率就是 B 连胜三局，也就是答案是 27 / 512。
对于频率学派来说，估计值本质上是利用数据构造出来的函数，既然数据是随机分布的，估计值肯定也是随机的。参数是确定的，数据是随机的，利用随机的数据推断确定的参数，得到的结果也是随机的。
贝叶斯学派 贝叶斯学派的核心是贝叶斯定理，用于计算后验概率。P(H|D)=P(D|H)⋅P(H) / P(D)。式中的 P(H) 被称为先验概率（prior probability）；P(D|H) 被称为似然概率（likelihood probability）；P(H|D) 被称为后验概率（posterior probability）。
在贝叶斯学派看来，5：3 的结果不能证明硬币正面朝上的概率更高，只能证明硬币正面朝上的概率更高的概率更高（仔细体会这句话），可能只是这次 B 运气不太好。因此，处理参数 p 的方式应该从变量的角度去观察，考虑所有可能的取值，再计算在所有可能的取值下 B 获胜概率的数学期望，从而消除 p 的不确定性对结果的影响。
换句话说，我们应当考虑 p 在不同的取值情况下的概率（请再次理解为什么 5:3 只能证明硬币正面朝上的概率更高的概率更高）分布，并以此来对不同 p 值情况加权。在这样的思想下，B 获胜的概率就可以写成 E=∫(1−p)^3 P(p|A=5,B=3)dp 利用贝叶斯定理可以求得结果为 0.</description></item><item><title>市场营销方法论</title><link>https://chenminhua.github.io/posts/2018_%E5%B8%82%E5%9C%BA%E8%90%A5%E9%94%80%E6%96%B9%E6%B3%95%E8%AE%BA/</link><pubDate>Thu, 20 Dec 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2018_%E5%B8%82%E5%9C%BA%E8%90%A5%E9%94%80%E6%96%B9%E6%B3%95%E8%AE%BA/</guid><description>最近在看美剧《广告狂人》，于是对广告业产生了一些兴趣，进而去了解了一下市场营销的方法论。Marketing 集成了商业的全部功能，并通过广告，销售以及其他市场活动直接和客户打交道。Marketing 是艺术与科学的混合，其中的知识非常广博。
市场营销过程是一个循环过程，一个市场计划的各个步骤和方面需要相互配合支持来达到目的。在市场计划中，做对一件事情是容易的，但是制定一个内部统一且相互支持的市场计划并不容易。整个市场营销过程主要包括以下几步。
1.客户分析 2.市场分析 3.竞争对手分析 4.分发渠道分析 5.制定营销组合 6.评估营销效益 7.重复1-6步并不断修正和扩展 客户分析 客户分析是市场营销的第一步。你必须先弄明白你的营销工作的对象是谁，以便对他们展开你的工作。
理清需求 需求类别是什么？谁需要？为什么需要？比如说电子烟产品吧。使用电子烟的人可能是为了戒烟，也可能是一名电子烟烟雾玩家。可能只是希望让自己看起来更酷一点。
理清购买者和用户 who is the buyer？ who is the user？ buyer 和 user 很可能不是同一个人，比如男士袜子的购买者常常是女性,所以在运动台推广袜子往往没什么用。
理清购买过程 购买过程可不是指用户是支付宝付款还是微信付款，而是指用户从产生需求想法到最终完成购买之后的整个过程。不同品类的产品，用户会有完全不同的购买过程。
比如你需要买一组旅行套装（小包装的牙刷牙膏肥皂等），你家门口正好有一个名创优品或者无印良品，你下班路过的时候直接走进店里，随手拿了一套就结账了。如果使用感受不错，你以后还会反复购买同样的产品。
比如你想买一个电动牙刷，可能是因为电视剧中的女主角使用电动牙刷的镜头吸引了你。然后你就去淘宝上搜了一下然后直接下单，或者看了两篇评测文章后做出决定。老实说，电动牙刷看起来都差不多，随便选一个品牌的价格较低的产品对你来说不会构成心理负担。大概一个小时内，你自己都没有意识到你已经下单了。
比如你想买一辆车，首先是你觉得你需要一辆车，而不是因为你路过了一个 4s 店。然后，你可能会咨询一些懂车的朋友，或者去知乎上看看高分的评论，或者去微博上随便搜搜看。终于你锁定了三到五款不同的车，他们各有利弊。A 车的油耗最低，但是外形不好看；B 车最有设计感，但是不太保值；C 车你很满意，但是稍稍有点超出你的预算了；D 车最近在促销感觉很划算，但是这个品牌的口碑不是太好。经过了反复地信息搜集和内心斗争。最终你决定买下其中的某一款。然后在开了两个月后，你开始慢慢评估自己的购买决定，比如我真的太蠢了，或者我真的太明智了。
总体来说，购买过程往往包括：先感知需求，再信息搜索，再评估替代产品，最后完成购买。其实还有一步，就是评估自己的购买决定。了解购买过程的一个主要方法是进行客户调查。
理清参与度 你需要弄明白，你卖的是一个高参与度的产品还是低参与度的产品？参与度可以用客户在购买过程中付出的精力和思考来衡量。通常来说，参与度和购买产品的风险有关。
如果客户在购买商品时感到较高的风险，那这就是一个高参与度的产品，比如买房子肯定是一个非常高参与度的事情。而买纸巾或者矿泉水则往往比较随机，用户对品牌甚至价格都是无感的。
提高用户参与度也对产品推广很有帮助。可以通过一些有参与度的广告（比如百事，耐克这些产品的广告）。可以给产品加入一些重要特性（killer feature）。
划分市场 在理清了上面的这些问题之后，你就可以划分市场了。 你可以选择按照地理位置划分，在北方投放羽绒服和取暖器广告的效果肯定比海南强的多。 你可以选择按照人口组成划分，比如划分婴儿、学生、中年人、老年人奶粉市场。
市场分析 市场分析步骤从更广的角度分析市场规模，发展趋势，竞争环境，行业监管环境。分析的目的是为了判断是否有必要投入精力在这个市场进行营销活动。市场分析需要弄明白三个问题：相关市场，生命周期，竞争因素。
相关市场 比如你打算向市场推出一种新的牛排产品，请注意，牛排市场不是你的相关市场。（下面的数据都是我乱讲的）假设全球牛排市场年销售总额是 1000 亿，其中中国市场占 10%，也就是 100 亿，而你的牛排产品打算在超市和电商售卖（零售），这部分市场核算下来大约有 50 亿，而且你的牛排产品属于 100 元以上一块的高价位牛排，这个细分市场算下来可能只剩 5 亿美元了。这才是你的相关市场。
生命周期（PLC） PLC 主要分为引入期，成长期，成熟期，衰退期。
引入期的时候大家还不太明白产品是什么，比如两年前的区块链。
成长期的时候大家都在问哪儿能买到，比如智能马桶盖这种东西。
成熟期的时候大家问为什么要买它，比如电动牙刷或者手机之类的产品。</description></item><item><title>聊聊微信读书</title><link>https://chenminhua.github.io/posts/2018_%E8%81%8A%E8%81%8A%E5%BE%AE%E4%BF%A1%E8%AF%BB%E4%B9%A6/</link><pubDate>Sun, 28 Oct 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2018_%E8%81%8A%E8%81%8A%E5%BE%AE%E4%BF%A1%E8%AF%BB%E4%B9%A6/</guid><description>这两天下了几个读书应用，发现市面上类似产品还真不少，比如微信读书，京东阅读，豆瓣阅读等等。我也深度体验了一下微信读书，这里谈谈个人的感受吧。
社交与用户 很多人都说微信读书试图用社交来撬动阅读，目前看来在这一点上，微信还是非常非常克制的，目前微信并没有大量采用群、朋友圈、小程序之类的流量武器给微信读书导流，但是却机智的使用了微信的通讯录，也就是说让用户在微信读书中看到他的朋友在看什么书（而不是微信中）。
微信读书也在试图突出用户在读书活动中的地位，并且带动一部分 ugc 内容与书本内容的整合。这是一件有点想象空间的事情，我们可以类比于音乐行业与网易云音乐，网易云虽然在版权上面输给了腾讯，但是其精心维护的社区和评论确实是同类产品中独一无二的，这也成为了他们的核心竞争优势。微信读书也有机会创造这样的一块 ugc 价值出来，并通过社交放大。
市场 读书的市场和音乐的市场能比吗？不好回答。但从感觉上看，音乐市场是完胜的。It’s not even close.
腾讯音乐在经历一系列整合之后，目前的估值已经达到了 300 亿美元，这简直就是一个天文数字，已经超过了当前网易的市值，离京东也不远了。那书这个行业呢？目前来看，阅文可能是目前最大的头部玩家，市值达到 430 亿港币，虽然和腾讯音乐还有一定的差距，但也是一个超出我想象的数字。
那么腾讯处在什么位置呢？等等，你仔细看下，阅文集团背后站着的，就是腾讯。腾讯对阅文的持股比重超过了 60%。腾讯音乐在整合音乐版权的时候，面对的上游通常是大型唱片公司；而网络书籍这一块呢，除了部分的出版商外，还有数不清的网络文学作家。我认为对腾讯来说，这简直是一场稳赢的战争，因为腾讯有阅文，他可以轻松的吃下这部分双边市场。但是需要注意一点，阅文的财报并不好看，相比于其市值，它的利润确实不怎么好看，市盈率破百，有很大的被高估风险。
目前阅文更多切入的是网络文学的市场，而读书市场绝对不仅仅这么简单。我觉得在线读书还有一块很大的可以切的市场是在线教育，比如像现在得到做的精读产品等等，这些产品的边际成本几乎为零，谁的流量成本低，谁就赚的多。那么，谁的流量成本能比微信低呢？
微信读书会改变什么？ 在微信读书上，你有两种不同的消费方式。你可以选择买一本书，钱货两讫，你付了钱，得到在这个平台上阅读这本电子书的权利。你也可以选择买一份微信读书的订阅(所谓的 netflix 模式)，此时你并没有得到任何书，但是你得到了在一定时间内在这个平台上阅读的权利。
这种订阅模式并不是什么新鲜玩意儿，相信大家各个平台的会员也都没少买。但是这种订阅模式从一定程度上，确实改变了供求关系。一个最简单的问题就是，平台如何给内容生产者付费？
平台可以选择买断式，也就是我一次性交付一大笔钱给你，购买你的作品的播放权阅读权，甚至我再多付一点钱，要求独家权利。这样一来，平台的定价权可能就会非常大。还有一种就是平台按照打开次数或者阅读时长等数据来计算费用，这种方式的话，作品价格就会和平台 kpi 直接绑定，谁给平台 kpi 贡献大，谁就多分钱。
无论如何，如果这种订阅模式成为书的主要渠道的话，都可能会对创作者的创作方式，创作质量产生巨大的影响。</description></item><item><title>敏捷开发</title><link>https://chenminhua.github.io/posts/2018_%E6%95%8F%E6%8D%B7%E5%BC%80%E5%8F%91/</link><pubDate>Wed, 12 Sep 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2018_%E6%95%8F%E6%8D%B7%E5%BC%80%E5%8F%91/</guid><description>在 2001 年，十七名软件开发人员在犹他州的雪鸟度假村会面，讨论这些轻量级的开发方法，并由 Jeff Sutherland，Ken Schwaber 和 Alistair Cockburn 发起，一同发布了敏捷软件开发宣言。
敏捷开发宣言 个体和交互 胜过 过程和工具 可以工作的软件 胜过 面面俱到的文档 客户合作 胜过 合同谈判 响应变化 胜过 遵循计划
敏捷开发就是在一个高度协作的环境中，不断地使用反馈进行自我调整和完善。
核心 迭代开发，价值优先。你需要经常性的交付对用户有价值的软件。工作的软件是首要的进度衡量标准，你应该使用短迭代，增量发布，不停交付你的软件，让客户做决定。
分解任务，真实进度。你需要分解你的项目工作，准确把控项目的进度。
站立会议，交流畅通。立会能够让团队之间交流更顺畅，彼此知道对方正在进行什么工作，进展如何，是否遇到一些障碍。最有效的交流方式就是面对面。
用户参与，调整方向。同 1，我们不能闭门造车，而是要和用户一起，让用户的需求反映进来。
结对编程，测试驱动，代码评审，保证质量。代码质量非常重要。
CI/CD。CICD 要尽早搭建起来，持续集成，持续发布，一切都要自动化。
定期回顾，持续改进。定期反省和调整。
不断学习，提高能力。关注优秀的技能和好的设计。 对团队投资，分享你的知识和你获得知识的方法。 定期举行读书会和讲座。 打破砂锅问到底。为什么用于比怎么做重要。
关于会议 会议一定要设立最终期限，防止陷入无休止的辩论。每个会议都要有一个负责人。
每天早上都要有立会。立会要足够简单有效，每个人需要回答三个问题：我昨天干了啥，我今天准备干啥，我遇到了什么阻塞。猪和鸡中，只有猪可以参加立会。
关于技术和代码 防微杜渐，不要只想着骗过测试。
如果你看到别人的一些代码很傻逼，重写掉它。
如果有人误解了需求，那这个误解可能会被传递下去。确保尽快消除这些误解。
指责不能修复 bug，我只关心解决问题。提出你的顾虑，而不是否定你的队友。维护你的队友，就算他犯了错，也不要让他在大家面前难堪。
代码集体所有制，每个人都可以改别人的代码。
要有一致的编码风格和代码规范，api 规范，非常重要。新人要先适应规则，再贡献代码。
架构师必须写代码，不要在 ppt 里面编程。主程应该试着承担架构师的角色。程序员拒绝设计，就是在拒绝思考。
架构师最重要的任务是：通过找到移除软件设计不可逆性的方式，从而去除所谓架构的概念。
记录问题解决日志。就像海盗的航海日记，飞行员的飞行手册一样。daylog 要可以搜索，可以 comment,记录问题（要详细，什么程序什么版本什么平台），debug 过程和解决方案。
关于团队和个人 围绕被激励起来的个体构建项目，给他们支持和信任。允许大家自己想办法。
可持续的开发速度，don’t burn out。
最好的架构，需求和设计出自自组织的团队。
帮助你的队友，让他们愿意来找你。对事不对人。 勇于承认自己不知道答案，这让人更放心。
不要试图说“这不是我的错”，如果你从来不犯错，那你可能也不是很努力。
如果你在的团队非常不职业，赶紧离开。
优秀的工程师并不拘泥于特定的任务，也不受公司信息和计算能力的约束。那些无论你是否批准都按自己想法做事的人，才值得你投资。
你不仅需要对你的团队抱有信任，也必须有足够的自信，才能给员工自由，让他们自己去寻找更好的答案。在物色领导人的时候，要挑选那些不会将一己之利置于企业利益之上的人。</description></item><item><title>需求变更的一致性</title><link>https://chenminhua.github.io/posts/2018_%E9%9C%80%E6%B1%82%E5%8F%98%E6%9B%B4%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7/</link><pubDate>Thu, 02 Aug 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2018_%E9%9C%80%E6%B1%82%E5%8F%98%E6%9B%B4%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7/</guid><description>最近在工作中遇到一次重大需求变更，对业务模型的设计也产生了非常大的影响。基本上就是在地基上挖个洞的那种变更。目前迭代还在进行中，但是我觉得是时候进行一些回顾，来思考这次大型迭代中我们做对和做错了哪些事情了。
产品一致性的重要性 复杂软件通常都会由几个不同模块组成。比如一个做包装功能的产品经理，突然改了一个需求，原本一个包装内只能装一种东西，现在要改成可以装不同的东西。当这个变更发生的时候，另一个可能使用包装的产品经理和相应的程序员都应该被告知，并修改相应的逻辑，并配合其一起上线。可是有时候，做底层功能的这个产品经理和程序员并不足够了解谁在依赖他们，可能就这么稀里糊涂开始写了，写着写着发现这个变更会影响到上层模块。而这时候上层模块正在经历属于它自己的迭代，一切就都搅和在一起了。
这种重新打地基式的迭代是非常糟糕的，你的团队会失去可用的共同语言，失去对产品的一致理解，失去相互间的信任。
所以，在项目建设的前期，做好需求调研和设计规划是最重要的。这能帮我们打好基础，让大家达成一定的共识。并且能够在产品方向出现分歧的时候作为一个指引。
但是，有些时候你很难在早期弄清楚用户到底需要什么，有时候避免不了会出现大的一些需求变更，甚至你会发现自己的模型根本就是错的。这时候你可能会做出一个产品迭代的决定，将现有的模型换掉。而这时候问题就来了。
你决定从底层的服务开始改起来，上层服务依赖着底层服务，所以上层服务也要改。而上层服务同时还有其他需求正在做，一切就都乱了套了。更可怕的是，你的服务已经上线了，这意味着已经有用户在使用你的软件。所以你要考虑 migration，你要考虑兼容性。于是你要在这段时间内需要搞定太多太多的事情了，或许模型本身并不复杂，但是因为这些问题，这样的迭代会非常困难。
这种不必要的复杂往往来源于产品的不一致性。当产品从一个合理的模型演变为另一个合理的模型的过程中，为了避免迭代周期过长，敏捷开发会将其拆分为一个一个细粒度的迭代。绝大多数情况下，敏捷开发都是好的。但是如果拆分不当，敏捷也会很危险。比如你把一个底层模块的变更安排在了一个迭代中，而上层模块先去写一些代码兼容它，code base 中会出现很多临时代码，如果这些代码由一些不熟悉这些业务的程序员来编写，或者由一些责任心不强的程序员来编写，就等于挖了一个坑。在这中情况下，应该尽可能让整个系统的功能保持一致性，宁可将底层模块的变更拆分到不同的迭代，也不要出现上下游服务不一致的情况。这需要团队内部充分的沟通，千万不能形成“安分守己”的团队文化。
程序员的责任到底是什么 程序员最重要的职责就是维护代码。这里的维护有两个含义，一方面是指让你的代码能够稳定正确地运行。更重要的一点在于，程序员应该为自己代码的整洁性和可维护性战斗。产品经理不会知道这个功能有多难做，你必须不停的告诉他，你必须习惯于不停地 challenge 产品经理，让他只做最重要最合理的需求。
一个好的技术团队，必须要相互挑战，产品经理站在用户的立场说话，程序员则必须为自己的代码战斗。如果程序员也站到了用户那边，代码就很可能越写越多越写越烂。很多时候，产品经理跑来跟你说一个需求，你乍一听挺合理，但是实现起来非常的困难，你一定不能直接答应下来。先推，让产品明白这东西很难做。再问产品为什么要这个功能，有没有其他方法满足他。产品经理往往会直接给你一个解决方案，但是优秀的程序员要去找到根上的那个问题。
总之，程序员的工作并不是完成产品经理的需求，而是开发并维护你的代码。这两者，从来都不是一回事。</description></item><item><title>三门问题</title><link>https://chenminhua.github.io/posts/2018_%E4%B8%89%E9%97%A8%E9%97%AE%E9%A2%98/</link><pubDate>Mon, 23 Jul 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2018_%E4%B8%89%E9%97%A8%E9%97%AE%E9%A2%98/</guid><description>背景 这是一个非常有名的问题，wiki。
简单说就是，有三扇门，一扇后面有奖，两扇后面有山羊。如果选手猜中有奖的门，就可以拿走奖品。 在选手做出选择之后，主持人（知道哪扇门后面有奖品）会选择选手没选的两扇中的一扇没有奖品的门打开。 这时候选手有一次改变自己选择的机会。
这时候，有人提出，选手改变选择能提高其获奖概率。起初这一观点遭到了强烈抨击。后来无数实验证明，这个观点是对的。 下面我们用 ebay 的一个开源库来证明一下，选手改变选择确实能提高其获奖概率。
实验 from bayesian.bbn import build_bbn def f_prize_door(prize_door): return 0.33333333 def f_guest_door(guest_door): return 0.33333333 def f_monty_door(prize_door, guest_door, monty_door): if prize_door == guest_door: if prize_door == monty_door: return 0 else: return 0.5 else: if prize_door == monty_door: return 0 if guest_door == monty_door: return 0 return 1 g = build_bbn( f_prize_door, f_guest_door, f_monty_door, domains=dict( prize_door = [&amp;#34;A&amp;#34;,&amp;#34;B&amp;#34;,&amp;#34;C&amp;#34;], guest_door = [&amp;#34;A&amp;#34;,&amp;#34;B&amp;#34;,&amp;#34;C&amp;#34;], monty_door = [&amp;#34;A&amp;#34;,&amp;#34;B&amp;#34;,&amp;#34;C&amp;#34;] ) ) 然后我们运行一下,假设选手选了 A 门，而主持人打开了 B 门</description></item><item><title>聊聊面向对象</title><link>https://chenminhua.github.io/posts/2018_%E8%81%8A%E8%81%8A%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/</link><pubDate>Sun, 15 Jul 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2018_%E8%81%8A%E8%81%8A%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/</guid><description>看到这个标题，可能很多朋友都会呵呵一笑。面向对象谁不知道？可是事实上，能正确理解和使用面向对象编程范式的人并不多，在不同语言背景下的程序员也往往对面向对象有不同的理解。我看过很多有一定经验的程序员写的代码，用着所谓面向对象的编程语言，写出来的代码却是根本无法维护的。为什么呢？因为他的代码没有隐藏该隐藏的信息，没有保护变化源。所以，OO 根本不是为了什么更好地模拟现实世界（那是给培训班用的卖点），而是为了隐藏信息，拥抱变化。
谈到 OO，就不得不提其三个基本特征：封装，继承，多态。下面我们就一起来看看这三个特性。
封装 一个比较流行的关于封装的定义是，隐藏对象的属性和实现细节，外部程序只能通过公开的接口来访问内部数据与方法。这种设计方式在硬件设计中是不言自明的，你拿到一个芯片，只需要阅读其说明书，知道每个管脚的作用，而不必知道其内部是如何实现的。在编程领域呢？封装是面向对象独有的吗？如何在不同的编程范式中实现封装呢？
首先，封装是面向对象独有的吗？
c 语言显然不是我们所认为的面向对象编程语言（尽管有一本很经典的编程书籍叫《面向对象 c 语言》）。但是事实上，c 语言有着非常好的封装特性。c 程序员喜欢将一堆方法签名写在一个头文件中，而需要使用这个模块的程序员会 include 这个头文件，事实上他们可能只能看到这个头文件。假设我们要实现一个栈。
struct Stack; typedef struct Stack Stack; Stack *newStack(); void destoryStack(Stack * stack); void * popStack(); void pushStack(void * ele); int sizeOfStack(Stack *stack); 使用这个栈的程序员看到的源代码只有这么多。他不知道你是怎么实现这个栈的。是用了数组还是链表？struct Stack 有哪些域？计算 size 的时候是遍历了所有元素还是在每次变更的时候去统计栈的长度？客户程序员对此一无所知。多么完美的封装啊。
c++可能是大多数程序员入门面向对象编程的语言了吧。在这里我们第一次学到了 public，private 这些个访问控制符。相比于 c 语言，我们开始拥有了类这个概念。c++程序员喜欢把类定义写在头文件里面，而将 c 语言里面的各种函数变成类的成员方法。c++之于 c 的另一个重大改变是泛型编程，但这不是本文讨论的重点。下面我们还是来看看栈这个例子。
template &amp;lt;class T&amp;gt; class Stack { private: std::vector&amp;lt;T&amp;gt; elems; // elements public: void push(T const&amp;amp;); // push element void pop(); // pop element T top() const; // return top element bool empty() const { // return true if empty.</description></item><item><title>feature vs function</title><link>https://chenminhua.github.io/posts/2018_feature_vs_function/</link><pubDate>Thu, 10 May 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2018_feature_vs_function/</guid><description>最近看到一段关于微服务的视频：拆分单体应用，其中有一段话引起了我的注意：split code by feature not by functionality。
大多数项目在一开始往往都只有一个后端服务。但是随着团队的扩展，业务需求的增加，单服务的可扩展性问题就会开始困扰你。如何拆分服务就成为了一个重要的问题。
关于上面那段话：split code by feature not by functionality。我查了一些资料，自己也做了一些思考，但是还是感觉很模糊。stackexchange 上的这个问题甚至被直接关闭了，理由是这个问题的答案很主观。
在日常的开发中，我们也常常会混用这两个词，加个 feature 和加个功能听起来是一回事，如果不分语境地强行去区分这两者，感觉没什么意义。但是在服务拆分的语境下面，区分这两个词或许是有意义的，甚至是有必要的。
function 更接近于描述我们想要的功能，比如记账的功能，汇率转换的功能，聊天的功能。而 feature 更接近于我们实现功能的方式，比如自动记账，手动记账，扫码记账。function 更为抽象，而 feature 更为具体。function 更稳定，而 feature 更易变。</description></item><item><title>spring aop中遇到的一个小问题</title><link>https://chenminhua.github.io/posts/2018_spring_aop%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%B8%AA%E5%B0%8F%E9%97%AE%E9%A2%98/</link><pubDate>Sat, 05 May 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2018_spring_aop%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%B8%AA%E5%B0%8F%E9%97%AE%E9%A2%98/</guid><description>最近做的一个项目使用了 spring+mybatis 的技术栈。实现很简单，在数据访问层写一系列 mapper 接口，定义一系列数据查询的方法。在服务启动时，让 spring 去扫这些接口，并为这些接口生成代理对象，也就是 DAO，这些 DAO 会实现 mapper 接口。同时这些 dao 都被 spring IOC 注入到相应的 service 中。
然后有一天，我们遇到了一个需求，某几个 mapper 中的某几个方法需要做一些特殊判断逻辑，并且需要改掉返回的数据。
一种方案是直接去改使用 mapper 的地方，但是以现有的架构来看，mapper 和 service 之间没有新的分层了，这会导致这个奇葩的需求需要写到 service 中的各个角落。如果有一天你想用删掉这些代码，或者修改这部分逻辑，后果不堪设想。
另一种方案是加切面，这样这部分逻辑就被集中到了代码主流程的外部，灵活度更高。
spring aop 有两种常用的给方法加切面的方式。一种是给方法加 annotation，另一种是用 execute()表达式。
起初我的想法是用 annotation 的方式。原因是这样更显式，在以后改代码的时候能够给自己一个提醒，其他伙伴改代码的时候也能知道这个方法会被拦截。于是我就写了个 annotation 加在 mapper 接口的某个方法签名上，然后去拦截这个 annotation 注解的方法。但是，拦截并没有生效。做了一番调查之后，我发现原来 annotation 不能从 interface 继承过来。参考 stackoverflow。简单来说就是@Inherited 只能在 superclass 上的注解才能起作用，而在接口上不起作用（不会往上追溯到接口）。这种设计的考虑可能是因为 java 单继承多实现的设计，如果去拿接口的 annotaion 可能会导致冲突。另外要注意的一点是，annotation 的继承并不能在运行时通过反射的方式直接在类上看到，而是通过类继承树向上查找的方式实现的。
如果换成使用 execute()表达式的方法，对既有代码的侵入性更低（有好处也有坏处），也确实可以解决问题。因为在拦截对象方法的时候，不用去查 annotation，而是直接找到实现该接口的指定方法去拦截。</description></item><item><title>相关与因果</title><link>https://chenminhua.github.io/posts/2018_%E7%9B%B8%E5%85%B3%E4%B8%8E%E5%9B%A0%E6%9E%9C/</link><pubDate>Wed, 02 May 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2018_%E7%9B%B8%E5%85%B3%E4%B8%8E%E5%9B%A0%E6%9E%9C/</guid><description>人类天生善于从噪声中寻找模式，这是一种与生俱来的能力。比如发现直角边的平方和等于斜边的平方，比如发现十二平均律以及五度音，比如发现一年有三百六十五天（公转周期）…
很多时候，人们会发现两件事往往同时出现，或者当一件事情发生时另一件往往不会发生。比如（下面都是我瞎说的）：
爸爸胖很可能儿子也胖。
喜欢跑马拉松的人很可能爱听 hiphop。
下雨天很可能堵车。
熬夜会容易长胖。
吸烟对环境有好处。
等等，最后一条是不是写错了？其实最后一条是我在 twitter 看来的，大概是说：吸烟会杀死人类，而人类对环境有害，所以吸烟有益环境保护。这一条论断看似非常离谱，但确是一条比较明确的因果推断。
而熬夜容易长胖这一条呢？一种解释方法是，熬夜的人比较容易吃零食，所以会容易胖（因果推断）。但是我们也可以说，工作压力大的人容易熬夜，工作压力大的人运动太少所以会长胖。这时候长胖和熬夜就变成了相关，换句话说，就算你现在不熬夜了，如果你工作压力还是很大，你依然会胖。
在统计学中，我们总是反反复复地和这两个概念打交道。甚至在统计学创建之前，我们就已经开始进行大量这方法的脑力活动–预测。
预测，在很多人看来是一种极度不靠谱的行为。我认同这种不靠谱，或者我们给它换一个好听点的名字：不确定性。这种不确定性是如此的确定，如此的科学，如此的贴近事物本质。尽管人们总是试图去对抗这种不确定性，但不可否认，它是不可战胜的。
但这依旧不能阻止人们去预测 &amp;mdash; 以一种更科学的方式。</description></item><item><title>一件让我愤怒的事</title><link>https://chenminhua.github.io/posts/2018_%E4%B8%80%E4%BB%B6%E8%AE%A9%E6%88%91%E6%84%A4%E6%80%92%E7%9A%84%E4%BA%8B/</link><pubDate>Wed, 18 Apr 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2018_%E4%B8%80%E4%BB%B6%E8%AE%A9%E6%88%91%E6%84%A4%E6%80%92%E7%9A%84%E4%BA%8B/</guid><description>最近在工作中遇到一些事情，让我有点不爽，或者说是愤怒。我觉得很多程序员根本称不上是 engineer，只能勉强算是个”terrible programmer”。
有些程序员不关心自己的代码，能不能被其他人看懂，会不会被别人错误地理解和使用。有些程序员甚至不关心自己的代码能不能被三天后的自己看懂。有些程序员其实根本不知道自己在写什么？他们只是凭借自己的感觉写完代码，然后扔给测试，然后等待测试提 bug，然后跑去问别人这边的逻辑应该是什么样的，然后用最“简单”的方式修复这个 bug（让这个 bug 不再发生）。
很多人都抱有这样的心态，甚至这种结果导向已经成为了一种政治正确。代码写的好坏不重要，赶紧通过测试，修好手上的 bug 上线才是最重要的。当然我同意修 bug 很重要（绝大多数情况下优先级最高），但是很多时候，大家都只想着怎么关掉这个 bug，而不会想想更重要的一些问题：这是不是一个 bug？这个 bug 是从哪来的？修复这个 bug 的方式会不会影响到其他逻辑?这个 bug 后面是不是还藏着别的坑？
如果你不去思考这些问题，而是快速看一眼 Jira,打开 debug 模式找到问题的原因(比如说，需要在某一段处理逻辑中加一段条件判断，在某种情况下跳过某几行代码的执行)。然后你想，oh，这个 bug 很好修，我只要多写个 if 然后在这种情况下跳过这几行代码就行了。你开心的写完这两行代码，提交并发布到测试环境，然后告诉测试那个 bug 被你修好了。接着去处理你还没处理的那十个新的 bug。
你觉得自己效率很高么？你错了，你糟糕透了，因为你失去了最适合思考和重构代码的时机。你被其他人催着跑，却丢掉了你最重要的东西。产品经理不会关心你的代码，测试不会关心你的代码，架构师不会关心你的代码，他们只要你把事情搞定。但是你必须关心你的代码，如果有必要的话，fight for your code!
如果需求要你写出难以维护的代码，你必须站出来发声，别人不会知道这东西有多坑，你必须非常严肃地告诉他们在技术实现上这是一个多么糟糕的主意。否则，你很可能为了一个优先级很低的需求，毁了你优雅的系统设计。
如果你不关心你的代码，那你永远也写不出好的代码，你的代码也不会成为好的产品。</description></item><item><title>十年</title><link>https://chenminhua.github.io/posts/2018_%E5%8D%81%E5%B9%B4/</link><pubDate>Wed, 11 Apr 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2018_%E5%8D%81%E5%B9%B4/</guid><description>github 十年了，这个被戏称为同性交友网站的章鱼猫，度过了十周岁生日。
十年前，我还在读高一，凯尔特人三巨头即将捧起奥布莱恩杯，利物浦在欧冠半决赛输给了切尔西，那时候我还不知道啥是程序员（那时的梦想是当一个 DJ）。当然，那时候也没有 github 和 stackOverflow(同样也创立于 2008 年)，不知道大家都去哪里抄代码。
2008 年的世界，已经遥远得无法想象。但是就像贝索斯说的“相比于未来十年会发生什么变化而言，未来十年什么不会变是一个更重要的问题”。那么 2008-2018，有什么没变呢？
朝鲜依然在搞核试验 java,php,c++依然是最受企业欢迎的编程语言 詹姆斯依然在骑士队（天真）
事实上，我发现想出几个没变的事物居然比想出几个变化的事物难很多。那么未来十年，在软件开发领域什么是不会变的呢？</description></item><item><title>聊聊抽象</title><link>https://chenminhua.github.io/posts/2018_%E8%81%8A%E8%81%8A%E6%8A%BD%E8%B1%A1/</link><pubDate>Sun, 08 Apr 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2018_%E8%81%8A%E8%81%8A%E6%8A%BD%E8%B1%A1/</guid><description>不要抽象。
上面这句是和一个架构师聊天时他说了二十遍的话。这句话对大多数程序员来说，都是非常反直觉的。可以说抽象是计算机软件设计中极为重要的一环，是每个程序员每天都要做的事情。当你在设计一个类来表达一组数据的时候，你正在完成一次抽象；当你设计出一个抽象类或者接口，你正在抽象你的抽象。
如果你是一个数学家，你应该会对“不要抽象”这四个字嗤之以鼻。数学家喜欢归纳和抽象，并从中发现数学之美。吴军在《数学之美》中甚至直接给出了一个论断：好的数学模型一定简单。
大量的数学模型都验证了这一点，欧拉公式，傅里叶级数，贝叶斯定理…所有这些公式在经过一系列推导和符号约减后，都成为一个可以被以最大号字体写在 T 恤胸口位置的公式。在我们为这些公式拍案叫绝的时候，我们往往忘记了：
That’s the good part of the world.
bad part 真实世界往往没有那么美好，很多事情未必能通过简单的推导和符号约简来简化。有些时候，我们会努力去识别问题中的一些 pattern，或者意外发现一些事物中的相关性。于是你惊喜地发现，只要变化一下看问题的角度，或者提出一些看起来很正确的假设，原本复杂的问题就会变得非常简单！
但是现实往往没有那么容易讨好，现实中的符号约简往往是一个非常危险的行为。因为你看问题的角度很可能不是最全面的，或者你的假设其实是错的。
很多时候，大脑处理现实复杂性的唯一方法是把庞大复杂的系统简化，这也是计算机编程中非常有用的机制。但是，如果将其视为理所当然，就会陷入符号简缪论。
在软件设计中，让分离的概念保持分离几乎是一条真理。尽管很多时候我们会遇到很多看起来相似的概念，让我们忍不住去合并它们，但是倘若引起它们改变的因素是不同的并且不可知的，那你很可能还是要将它们拆开的（很快）。所以有些时候，费力进行抽象带来的边际效益会很低。
感兴趣的朋友可以看看这个演讲 prefer duplication over the wrong abstraction
我们依然需要抽象 回到文章开头那句话：不要抽象。其实我依旧并不同意这句话。尽管那位前辈说：你还年轻，等你挖的坑够多之后你就明白了。
抽象是危险的，一味追求简单而完美的模型会把我们带入误区。但如果因此放弃抽象，将现实的 bad part 直接带入我们设计的系统中，更是一种投鼠忌器的表现，甚至可以说是偷懒并且不负责任的设计。保持灵活的最好方式是少写代码，而在概念产生明显分化之前，采用相对抽象的概念确实能够帮助你少些很多代码。除此之外，当我们采用“非抽象”方式完成部分系统设计和代码编写的过程中，不断审视自己过去的想法，寻找 pattern，也能帮助我们构建更易于理解和维护代码。
其实抽象也好，不抽象也罢，和其他系统设计的问题一样，我们都可以用一个看起来没什么用但是永远正确的词来回答：balance。你要去平衡开发团队的能力，合作者（需要理解这些概念的人）的数量，对系统边界的影响，开发时长，以及灵活性（能否推翻你当前的选择）等等所有的因素，最终决定抽象的层级。
拥抱抽象，但要适可而止。</description></item><item><title>如何告诉一个外星人什么是树</title><link>https://chenminhua.github.io/posts/2018_%E5%A6%82%E4%BD%95%E5%91%8A%E8%AF%89%E4%B8%80%E4%B8%AA%E5%A4%96%E6%98%9F%E4%BA%BA%E4%BB%80%E4%B9%88%E6%98%AF%E6%A0%91/</link><pubDate>Sat, 07 Apr 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2018_%E5%A6%82%E4%BD%95%E5%91%8A%E8%AF%89%E4%B8%80%E4%B8%AA%E5%A4%96%E6%98%9F%E4%BA%BA%E4%BB%80%E4%B9%88%E6%98%AF%E6%A0%91/</guid><description>在《认知开发潜能》一书中，我曾读到这样一段关于树的话，大意是：你可以把一棵树看作一个单独的，离散的对象。但事实上，一棵树至少由两个主要系统连接：树叶和空气的处理循环与根和泥土的处理循环。这段话让我开始思考一个问题，我们应该如何告诉一个外星人什么是树?
让我们一起来试试吧。首先，
树是一种植物。 主要组成部分是根、干、枝、叶、花、果。
那树有什么特性呢？
通常长在泥土里。 生长需要水和阳光。 通过叶子进行光合作用（又一个难以解释的概念）。
树是从哪来的？
balabala 进化来的吧。。。（生物学的不好）
方法论 如同我们讨论树的方式一般，通常我们可以从定义、特性、实现、场景、历史、相关、抽象等不同角度去看待事物。下面我们再试试解释下后朋克好了。
后朋克(post punk)是一种80年代极为流行的音乐流派。 后朋克相对于传统摇滚乐而言更为实验，音乐更另类也更艺术化。 以the cure和new order等乐队为代表的80年代后朋乐队都非常擅长使用合成器技术， 并且作曲非常好听（不像某些朋克乐队）。 后朋克在80年代初流行，但是在80年代中期逐渐走向没落，并逐渐转向了另类摇滚。 后朋克和朋克的关系就像javascript和java的关系一样--没啥关系。 事实上和后朋克常常混起来的是新浪潮和另类摇滚，大多数时候，区分他们也没什么意义。</description></item><item><title>move slow and mend things</title><link>https://chenminhua.github.io/posts/2018_move_slow_and_mend_things/</link><pubDate>Fri, 06 Apr 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2018_move_slow_and_mend_things/</guid><description>“move fast and break things” 这是 fackbook 著名的 motto，意思是说，在使用新技术和新工具的时候，尽管其可能会有不稳定的地方，但是为了追求开发速度，应当果断使用那些能够提升开发效率的新技术。由于有了 facebook 的成功背书，很多初创公司都将其引为自己公司的开发宗旨。
但是，很多人都误解了这句话，认为只要短期内 move fast 了，就可以随意的 break things，事实上这样做只会让你走得更累也更慢。更重要的一点是著名的破窗理论：“如果你的一扇窗户破了，你不去修复，很快其他窗也会跟着破，房子会变得一片狼藉”。很多人认为为了 move fast，就可以稍微放弃一点点对软件质量的要求。但是这样的妥协就像是纵容了一扇破窗，软件会快速腐坏并让你崩溃。
我也曾经历过由于项目进度紧张而导致没有时间重构代码，没有时间 review 项目总体设计的情况。实际教训告诉我，宁可加班，宁可项目延期，也不能放弃对代码质量的要求。我司技术 vp 曾经跟我说，快是衡量一个技术团队的唯一标准。但我相信，这一论述永远要加上保证软件质量的前提。
事实上前两年，facebook 也把他们的 Motto 改成了”move fast with stable infra”。你看，facebook 也受不了工程师随便 break things 了。
有些时候，与其”move fast and break things”不如”move slow and mend things”。</description></item><item><title>数学与经济</title><link>https://chenminhua.github.io/posts/2018_%E6%95%B0%E5%AD%A6%E4%B8%8E%E7%BB%8F%E6%B5%8E/</link><pubDate>Fri, 06 Apr 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2018_%E6%95%B0%E5%AD%A6%E4%B8%8E%E7%BB%8F%E6%B5%8E/</guid><description>数学的优势在于，它需要精确定义，而且为多样化的领域提供了共同语言，但是它也有局限性。公式与其基本假设，往往夸大了可以从经济理论预测出来的精密度。比起任何实际的经济而言，许多经济理论，更多关乎一个完全虚构的世界。经济模型可能成为空中楼阁，结构精巧而一无是处。
当一个经济理论被表达为一个数学公式，就事先假定了它是公正的，而事实往往并非如此。
亚当·斯密在出版于 1776 年的经典著作《国富论》中，提出了一个非常引入注目的理论——自由市场理论。通过简化假设，他将自由市场的运作描述为受供给和需求力量驱动的典型市场。在这一 18 世纪的伟大经济模型中，他提出了价格将起到核心作用，因为价格表示了稀缺或盈余，并将达成理想的市场状态。亚当·斯密的主要观点是，自由市场可以有效调节生产、分配，就像一只看不见的手。在亚当·斯密的描述中，政府通常如同恶魔，干扰价格信号，扰乱自由市场的均衡机制。
新古典经济学家简单地认为：人的理性行为是一致的、可预测的、可靠的，并且深深植根于人的自身利益。同时他们还认为，人的理性行为假设在经济模式中很普遍，它可以用术语“经济人”来代表，即结合所有这些特性的一种神秘动物。
支持自由市场理论的经济学家更有可能认为人是完全理性的，而且他们的行为是在掌握了完全的信息和客观性的前提下做出的。自由市场理论的怀疑者认为，行为和条件不够完美时，自由市场理论也不太可能完美。凯恩斯的研究起点不是完美市场运作的假设，而是基于 20 世纪 30 年代“大萧条”这一现实，他的理论不受限于“经济人”或者其他新古典经济学假设。
有趣的是，这两种经济学家都可能在数学上达成正确的结论，而得出完全矛盾的经济学理论。更有趣的是，这两种经济学家都在过去 40 年内被授予了诺贝尔奖。</description></item><item><title>质数</title><link>https://chenminhua.github.io/posts/2018_%E8%B4%A8%E6%95%B0/</link><pubDate>Mon, 02 Apr 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2018_%E8%B4%A8%E6%95%B0/</guid><description>突然想知道第 n 个质数是多少，比如说我的生日是 11 月 24 日，那么属于我的那个质数是多少呢？
先写个判断一个数是不是质数的函数
bool isPrime(int n){ if (n == 1){ return false; }else { for (int i = 2; i &amp;lt;= floor(sqrt(n)); i++){ if (n % i == 0){ return false; } } return true; } } 然后调用它
int main() { int n; cin &amp;gt;&amp;gt; n; int count = 0; int j = 2; while (true) { if (isPrime(j)){ count += 1; cout &amp;lt;&amp;lt; j &amp;lt;&amp;lt; endl; if (count &amp;gt;= n) { break; } } j += 1; } } 结果显示第 1124 个质数为 9043。</description></item><item><title>akka</title><link>https://chenminhua.github.io/posts/2018_akka/</link><pubDate>Sun, 01 Apr 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2018_akka/</guid><description>akka 不是一个 framework，而是一个 toolkit 或者说运行时，用于在 jvm 上构建高并发、分布式、弹性、消息驱动的应用。Actor 执行操作来响应消息。这些操作包括更改 actor 自己的内部状态，以及发出其他消息和创建其他 actor。所有消息都是异步交付的，因此将消息发送方与接收方分开。这种分离，使得 actor 系统具有内在的并发性：可以不受限制地并行执行任何拥有输入消息的 actor。
像真实世界的演员一样，Akka actor 也需要一定程度的隐私。您不能直接将消息发送给 Akka actor。相反，需要将消息发送给等同于邮政信箱的 actor 引用。然后通过该引用将传入的消息路由到 actor 的邮箱，以后再传送给 actor。Akka actor 甚至要求所有传入的消息都是不可变的。
与一些真实世界中演员的需求不同，Akka 中由于某种原因而存在一些看似强制要求的限制。 使用 actor 的引用可阻止交换消息以外的任何交互，这些交互可能破坏 actor 模型核心上的解耦本质。 Actor 在执行上是单线程的（不超过 1 个线程执行一个特定的 actor 实例），所以邮箱充当着一个缓冲器，在处理消息前会一直保存这些消息。 消息的不可变性意味着根本无需担心可能影响 actor 之间各种共享的数据的同步问题。
actors sending messages changing its state changing its behavior creating more actors
class Activity class Disable(val password: String) class Enable(val password: String) class Alarm(val password: String): AbstractLoggingActor() { override fun createReceive(): Receive { return disabled } val enabled = receiveBuilder() .</description></item><item><title>并发与并行</title><link>https://chenminhua.github.io/posts/2018_%E5%B9%B6%E5%8F%91%E4%B8%8E%E5%B9%B6%E8%A1%8C/</link><pubDate>Thu, 29 Mar 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2018_%E5%B9%B6%E5%8F%91%E4%B8%8E%E5%B9%B6%E8%A1%8C/</guid><description>很多初级程序员都会把并发和并行搞混在一起，或者认为并发和并行根本就是一回事。而其实并发和并行压根就不是在讨论一个问题。本文我们就一起来看看究竟什么是并发和并行，以及如何实现并发和并行。
并发与并行 并发（concurrency）用于描述问题。如果一个问题可以被拆分成多个问题进行局部求解，或者更本质的说，如果每个局部的子问题的求解都不因其求解的顺序改变而改变，那么这就是一个可并发的问题。（再次注意：并发描述的是问题）
并行描述的是解决问题的方法。二十个人一起搬砖也好，二十个 CPU 核一起计算也好，并行描述的就是将任务分配给多个可以执行任务的执行者，分别执行任务的方式。
举个简单的例子。大家都应该去过图书馆借书，假设现在图书馆有一个前台管理员，A 同学去前台借书，同时 B 同学去前台还书，C 同学在咨询某个问题。这时候，图书管理员可以同时处理你们的”请求”，先从 A 手中接过书，然后回到 C 同学的问题，再用扫码枪扫 B 同学的书，然后帮 A 同学办理借书登记。整个过程中，管理员执行的任何一个子任务都不会因为其处理的先后顺序而导致其处理结果的变化。因此我们说，图书管理员的工作是一个可并发的工作（可以”同时”处理多个任务）。事实上，类比到服务端开发上，图书管理员不过就是一个开放了若干接口（借书，还书，咨询等）的服务器么。
而随着来图书馆的同学越来越多，尽管图书管理员依旧面对一个可并发的问题，但是其处理问题的速度（计算能力）是有限的，而不停在不同任务之间切换（上下文切换）更增加了他处理问题的时间成本。于是图书馆决定增加两名前台管理员，这就是并行处理问题的方式。
后端服务通常都是在处理可并发的问题。在不增加计算能力的情况下，通过增加线程（这里的线程是一个抽象的概念，可以理解为一组具有独立上下文的计算资源）的方式可以让服务能够更好的处理并发问题（同时处理多个任务）。当某个任务需要占用较长时间，但并不占用 cpu 时，服务可以切换去其他任务，让 cpu 先去处理其他任务，之后再回来。
矩阵乘法 我们再换个例子，google 引以为傲的 page rank 算法中最关键的一步就是计算两个矩阵的乘积，然后得到不同网页之间的相关性。但是现在的互联网太大了，网页数都是以亿为单位的。计算两个超大矩阵的乘积显然是一个非常耗时的工作。如果用一台计算机算显然是不可能的。
事实上这压根不是一个计算机问题，而是一个矩阵乘积计算的数学问题。简单来说，两个矩阵的乘积可以由其各自的部分子矩阵的乘积组合而乘（非常基础的线性代数知识）。假设我们要计算两个 300,000,000 _ 300,000,000 的矩阵的乘积，我们可以将其分别划分为 9 个 100,000,000 _ 100,000,000 的矩阵，一共需要进行 27 次 100,000,000 * 100,000,000 的矩阵的乘法和若干次矩阵加法（相比于矩阵乘法而言，矩阵加法的时间复杂度要低很多，可以暂且忽略）。
现在我们的超大矩阵乘法就变成了 27 个稍微小一点的矩阵的乘法了。尽管看上去没什么了不起，但是这个特性却是决定性的，因为我们可以把矩阵乘法拆分到不同的机器上计算，再集合到一起就行了。事实上，你可以继续拆分，把 100,000,000 _ 100,000,000 的矩阵继续拆分成 100 个 10,000,000 _ 10,000,000 个矩阵，这时候问题就变成了 27000 个 10,000,000 * 10,000,000 的矩阵的乘积。
如果你有 27000 台服务器的话，这个超大矩阵计算的时间复杂度就降低了 27000 倍！！原本需要计算一个月的问题现在只需要一分半钟就可以解决。</description></item><item><title>About</title><link>https://chenminhua.github.io/about/</link><pubDate>Tue, 27 Mar 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/about/</guid><description>I code whatever I like.
chenmhgo@gmail.com</description></item><item><title>The rule of hole</title><link>https://chenminhua.github.io/posts/2018_the_rule_of_hole/</link><pubDate>Tue, 27 Mar 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2018_the_rule_of_hole/</guid><description>The rule of hole: If you fall into a hole, don&amp;rsquo;t dig.
对很多人来说，承认自己错了并不是一件容易办到的事情，即使他们看上去并不是一个固执的人。有时候是因为害羞，有时候是为了面子，有时候是为了某些利益，有时候是觉得自己能扭转局面，人会在意识到自己错了之后，继续沿着原路往前走，越陷越深。
这种心态可以用“沉没成本”理论来解释，说白了，就是我已经走了这么远了，现在掉头的话，前面做的都白做了。前面做的所有努力，都成为了这一刻选择掉头的“沉没成本”，而你走得越远，“沉没成本”就越高。
关于如何成为一个职业的程序员，我们常常被教育要说到做到。能就是能，不能就是不能，不要说试试看。当然，我完全同意这些话，但是有些同学会把这句话理解为，如果我接了这个需求，我就一定要做到。
上面的两种说法看起来差不多，其实换了个角度，表达的意思就完全不同了。在评估需求的时候，我们当然要足够认真仔细，明白什么是你能做到的，什么是你做不到的，什么是你能做到但是让你觉得不对劲的。（如果你只把自己当做一个勤劳的码农，觉得评审需求是产品经理和架构师的事，那你并不是一个合格的工程师。如果你们公司不让你参加需求评审，那还是趁早换个地方吧。）但是当你在开发过程中，发现之前评审的需求中某个地方有逻辑漏洞，或者你发现你必须搞一堆很奇怪的东西（在代码里下毒）才能完成需求，你应该立即站出来，而不是闷着头做下去。
这是我在工作中学到最重要的一件事：掉坑别挖。</description></item><item><title>copy on write</title><link>https://chenminhua.github.io/posts/2018_copy_on_write/</link><pubDate>Sat, 24 Mar 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2018_copy_on_write/</guid><description>swift 中的 cow swift 中有 struct 和 class 这两种数据结构，很多入门教程都会介绍说，struct 和 class 最大的区别是，struct 是值类型，而 class 是引用类型。换句话说，struct 在传递的过程中是值传递，而 class 传的则是引用。那么为什么需要这两种传值机制呢？
从并发角度看，值传递似乎是一种更安全的选择。因为值传递在每次传递过程中，对象都会被拷贝一份，这样可以防止对象被意外修改。 但是在很多情况下，或许用户确实需要一个对象在不同的执行块共享，甚至共同修改，这时候引用传递显然更适合。 更重要的是，引用传递是一种开销更小的传递方式，程序不需要为这次传递申请一块新的内存并拷贝对象，也不用考虑对象失去全部有效引用时回收这块新开辟的内存。 那有没有可能实现一种技术，让我们同时享受值传递和引用传递的优点呢？答案是”写时复制”(cow, copy-on-write)。顾名思义，cow 就是说当使用值传递的时候，在新的代码块中依旧引用原来的对象，直到改变(写)对象的时候，才复制一份对象并引用新对象。在 swift 中，你可以使用 mutating 关键字来标识 struct 的某个方法是否会改变这个对象。当调用一个 mutating 方法时，才会真的复制一份对象。
cow 与引用计数 英文好的同学建议直接读这篇 instagram 的原文 「在 instagram 停用了 python 的垃圾回收」。
linux 内核有一个这样的机制：子进程启动时会共享父进程的内存页，但是只有当子进程试图去写这些内存页的时候，才会把这些内存拷贝到新的内存页，也就是我们说的”写时复制”。但是在 python 中，由于 python 使用引用计数来进行垃圾回收，这导致了每次读一个对象，都需要去写这个对象的引用数(加 1)，所以所有的读就都变成了写，cow 机制就失效了。为此，instagram 关掉了 python 的 gc…</description></item><item><title>我只是想要一个函数啊</title><link>https://chenminhua.github.io/posts/2018_kotlin%E4%B8%AD%E7%9A%84%E5%8F%AF%E7%A9%BA%E7%B1%BB%E5%9E%8B/</link><pubDate>Fri, 23 Mar 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2018_kotlin%E4%B8%AD%E7%9A%84%E5%8F%AF%E7%A9%BA%E7%B1%BB%E5%9E%8B/</guid><description>kotlin 中的可空类型 最近在公司使用 Kotlin 写后端服务（spring + mybatis），遇到 optional 的小问题，如下
fun getProject(projectCode: String): ProjectDO = projectMapper.getProjectByCode(projectCode) 上面这个方法根据项目号去数据库拿项目，从类型签名上看，getProject 方法返回的是一个 ProjectDO 类型的对象，而且由于不是可空类型，所以不会是 null。但是，如果数据库里面确实没有这个项目号的项目呢？试验一下
print(getProject(&amp;#34;no such project&amp;#34;)) // null 返回结果是 null!!
kotlin 欺骗了我们，明明说好是非空，怎么就返回了一个 null 给我!!
其实这锅 kotlin 真不背。了解 mybatis 的同学应该知道，mybatis 采用的是 jdk 代理的模式来代理 Mapper 接口。尽管我们在定义 mapper 接口时写明了返回非空类型的对象，
projectMapper.getProjectByCode(projectCode): ProjectDO 但是在 jvm 上运行的时候，运行时类型可没有什么非空不非空的。查不到数据，mybatis 的 mapper 的代理对象就返回了一个 null 给调用者，调用的地方拿这个对象赋值也好，返回也好，也不存在可空类型，拿到 null 就是 null 了。
所以 kotlin 的可空类型，只能在编译时帮你搞定一些程序员搞出来的空指针异常。而在一些使用了代理技术的地方，运行时还是会给你跑出一个 null 来，仍然需要你手动处理。
by the way，如果你阅读 kotlin 代码编译后的字节码，会发现其实 kotlin 在所有不是可空的变量上加上了@NotNull 注解，其 RetentionPolicy 为 class</description></item><item><title>我只是想要一个函数啊</title><link>https://chenminhua.github.io/posts/2018_%E6%88%91%E5%8F%AA%E6%98%AF%E6%83%B3%E8%A6%81%E4%B8%80%E4%B8%AA%E5%87%BD%E6%95%B0%E5%95%8A/</link><pubDate>Mon, 19 Mar 2018 20:00:08 +0800</pubDate><guid>https://chenminhua.github.io/posts/2018_%E6%88%91%E5%8F%AA%E6%98%AF%E6%83%B3%E8%A6%81%E4%B8%80%E4%B8%AA%E5%87%BD%E6%95%B0%E5%95%8A/</guid><description>从面向对象说起 显然，面向对象编程在很长一段时间内，都是最主流的一种编程范式。那到底什么是面向对象呢？为什么我们需要对象呢？
面向对象的最重要特性有三个：封装，继承，多态。
在我看来，多态无疑是这三者中最为重要的特性，它解决了软件架构中最为复杂的依赖问题，也就是我们常说的解耦。 继承则解决了代码复用的问题，并实现了一定程度的抽象。 而封装则使数据变得更安全，同时也隐藏了数据的存储结构与类型。 对象的本质 假设我们现在要写一个人员管理的软件，需要实现一个给人员年龄加 1 的功能。非面向对象的写法可能是
addOneYearAge(person) 我们拿到一个数据的引用，然后给其年龄加一。而面向对象的方式则可能是
person.growUpOneYear() 这里我们有一个对象，然后给对象发送了一个消息，通知它发生一些变化。
这两种写法都能实现我们的目标，那么那种更好呢？如果我们抛开运行性能，抛开运行时函数入口跳转等等奇怪的字眼，单纯的看看上面两段代码，我觉得他们是一样的（我相信肯定有人会说第二种更好，或许也有人会支持第一种）。但是，当你的数据有二十种改变的方式的时候，第一种写法会崩溃的更快，各种改变 person 的函数会被写在你永远想象不到的地方，而第二种写法则相对容易管理，因为你控制了数据的改变（如果你知道你在做什么的话）。
所以，对象是什么已经很显而易见了。对象是一堆数据的集合，以及可以操作这些数据的方法。
闭包 为什么要从面向对象扯到闭包？这两个东西八竿子打不着啊？
从前我也是这么认为的，直到有一天我读了 ruby 作者松本行宏的书《代码的本质》。他说：对象和闭包是同一事物的两面。下面是一段简单的 js 中使用了闭包的代码。
function outside() { var a = 0; return function inside() { a++; console.log(a); }; } var f = outside(); f(); //1 f(); //2 f(); //3 我们可以看到 outside 函数返回了一个 inside 函数，inside 函数可以访问 outside 函数作用域内声明的变量 a。
通常情况下，我们认为当函数返回时，其调用栈的栈帧(stack frame)以及其上的变量就会被销毁，所以当 outside 返回的时候，a 就应该不能访问了，但事实是 inside 函数可以始终访问到 a 变量。关于闭包可能导致的内存泄露问题不是本文讨论的重点。真正让我觉得有趣的依然是松本先生的那句话：对象和闭包是同一事物的两面。
其实，我们可以认为对象是通过类机制将过程（方法）封装到了数据里面，而闭包是通过作用域将数据封装到过程（方法）里面。它们的本质都是将数据与操作数据的过程（方法）放到一起，将数据与计算放在一起罢了。
(如果你还是不能理解这段内容，请仔细阅读《sicp》的第二章。)</description></item></channel></rss>