<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>身高背后的统计学 - 硕大的汤姆</title><meta name=viewport content="width=device-width,initial-scale=1"><meta itemprop=name content="身高背后的统计学"><meta itemprop=description content="给同年龄的一百个小孩量身高，已经量了 99 个，请问最后一个小孩的身高有 99%的概率会小于多少。
我们只知道这些数据来自同一个总体（服从同一个分布），前 99 个小孩和最后一个是一视同仁的，也就是说，这一百个小孩每个都有 1%的可能是最高的。所以最后那个小孩的身高有 99%的概率小于前面 99 个小孩中最高的那个。
而一些同学可能会有不同的观点，通常来说同龄小孩身高会服从正态分布。通过分析前 99 个小孩的身高我们是可以算出这个正态分布的参数的。假设均值为 100，方差为 4，则可以很容易算出最后一个小孩以 99%的概率低于 100 + 2 * 2.365 = 104.73 厘米。
参数模型与非参数模型 （parametric/non-parametric model） 上面两种方法中，第一种是非参数模型，它并不关心这些数据属于什么分布。第二种则是参数模型，其依赖于一个很强的先验假设：身高服从正态分布。换句话说，如果模型有一组固定的参数来描述其概率分布，则为参数模型；否则为非参数模型。
参数模型的优点是只有几个少量的参数就刻画了整个模型，每个参数都有其明确的统计意义（比如上面的均值和方差），但是它需要依赖于超强的先验假设：所有数据符合特定类型的概率分布。学习的过程就是利用训练数据估计未知参数的过程，这些估计出来的参数就是训练数据的浓缩。在这个过程中，先验知识确定了假设空间的取值范围，学习算法（比如最大似然估计或是最大后验概率估计）则在给定的范围内求解最优化问题。
可如果先验分布本身就不符合实际（换句话说你的假设空间错了），学习算法再强，数据再好也得不出正确结果。所以当对所要学习的问题知之甚少的时候，应当避免对潜在模型做出过多的假设。这类不使用先验信息，完全依赖数据进行学习得到的模型就是非参数模型。
非参数模型不是“无参数模型”，恰恰相反，非参数模型意味着模型参数的数目是不固定的，并且极有可能是无穷大，这决定了非参数模型不可能像参数模型那样用固定且有限数目的参数来完全刻画。在非参数模型中不存在关于数据潜在模式和结构化特性的任何假设，数据的所有统计特性都来源于数据本身，一切都是“所见即所得”。当训练数据趋于无穷多时，非参数模型可以逼近任意复杂的真实模型，而代价是非常大的时空复杂度。
归根结底，非参数模型其实可以理解为一种局部模型，就像战国时代每个诸侯国都有自己的国君一样，每个局部都有支配特性的参数。在局部上，相似的输入会得到相似的输出，而全局的分布就是所有局部分布的叠加。相比之下，参数模型具有全局的特性，所有数据都满足统一的全局分布，这就像履至尊而制六合得到的扁平化结构，一组全局分布的参数支配着所有的数据。
数据模型(data model)与算法模型(algorithm model) 从数据分布的角度看，不同的模型可以划分为参数模型和非参数模型两类。如果将这个划分标准套用到模型构造上的话，得到的结果就是数据模型（data model）和算法模型（algorithm model）。相比于参数对数据分布的刻画，这种分类方式更加侧重于模型对数据的拟合能力和预测能力。
http://www2.math.uu.se/~thulin/mm/breiman.pdf 统计模型，两种不同的文化。
作为一个统计学家，布雷曼看重的是学习算法从数据中获取有用结论和展示数据规律的能力。从这一点出发，他将从输入 xx 到输出 yy 的关系看成黑盒，数据模型认为这个黑盒里装着一组未知的参数，学习的对象是这组参数；算法模型则认为这个黑盒里装着一个未知的映射 f()˙f()˙，学习的对象也是这个映射。
数据模型和参数模型类似，其最典型的方法就是线性回归。线性回归的含义明确而清晰：输入数据每个单位的变化对输出都会产生同步的影响，影响的程度取决于这个特征的权重系数，不同特征对结果的贡献一目了然。
算法模型和非参数模型类似，其著名代表就是随机森林算法。随机森林是一种集成学习方法，构成这座森林的每一颗树都是决策树，每一棵决策树都用随机选取数据和待选特征构造出来，再按照少数服从多数的原则从所有决策树的结果中得到最终输出。
如果说参数模型与非参数模型的核心区别在于数据分布特征的整体性与局部性，那么数据模型和算法模型之间的矛盾就是模型的可解释性与精确性的矛盾。数据模型有更好的可解释性，而算法模型则相对而言抛弃了可解释性（对于特别复杂的问题，追求可解释性几乎是不可能的，追求精确性更重要）。
决策树本身是具有较好可解释性的数据模型，它表示的是几何意义上对特征空间的划分，但是精确度却不甚理想。随机森林解决了这个问题：通过综合使用建立在同一个数据集上的不同决策树达到出人意料的良好效果，在很多问题上都将精确度提升了数倍。但精确度的提升换来的是可解释性的下降。每个决策树对特征空间的单独划分共同织成一张剪不断理还乱的巨网，想要理解这张巨网背后的语义无异于水中望月、雾里看花。
生成模型与判别模型 还有另一种针对学习对象的划分方式，那就是生成模型和判别模型之分。简单地说，生成模型学习的对象是输入 xx 和输出 yy 的联合分布 p(x,y)p(x,y)，判别模型学习的则是已知输入 xx 的条件下，输出 yy 的条件分布 p(y|x)p(y|x)。两个分布可以通过贝叶斯定理建立联系。
生成模型和判别模型的区别可以这样来理解：假如我被分配了一个任务，要判断一个陌生人说的是什么语言。如果用生成模型来解决的话，我就需要把这个老外可能说的所有语言都学会，再根据他的话来判定语言的种类。但可能等我学完这些语言时，这个陌生人都说不出话了。可是用判别模型就简单多了，我只需要掌握不同语言的区别就足够了。即使不会西班牙语或者德语的任何一个单词，单凭语感也可以区分出这两种语言，这就是判别模型的优势。
针对生成模型和判别模型的利弊，支持向量机的奠基者弗拉基米尔·瓦普尼克（Vladimir Vapnik）有句名言：“（解决分类问题）应该直截了当，不要用兜圈子的方式，搞一个更难的问题（比如求解似然概率）做为中间步骤”。一般来说，生成模型的求解更加复杂，当数据量趋于无穷大时，渐进条件下的精确性也更差，但其收敛的速度更快，在较少数据的训练后就可以收敛到错误的下界。相比之下，判别模型的形式更加简单，在分类问题上的表现也更出色，却不能提供关于数据生成机制的信息。有些情况下，生成模型和判别模型会成对出现。例如在分类问题中，朴素贝叶斯和逻辑回归就是一对生成 - 判别分类器。"><meta itemprop=datePublished content="2019-03-08T20:00:08+08:00"><meta itemprop=dateModified content="2019-03-08T20:00:08+08:00"><meta itemprop=wordCount content="68"><meta itemprop=keywords content><meta property="og:title" content="身高背后的统计学"><meta property="og:description" content="给同年龄的一百个小孩量身高，已经量了 99 个，请问最后一个小孩的身高有 99%的概率会小于多少。
我们只知道这些数据来自同一个总体（服从同一个分布），前 99 个小孩和最后一个是一视同仁的，也就是说，这一百个小孩每个都有 1%的可能是最高的。所以最后那个小孩的身高有 99%的概率小于前面 99 个小孩中最高的那个。
而一些同学可能会有不同的观点，通常来说同龄小孩身高会服从正态分布。通过分析前 99 个小孩的身高我们是可以算出这个正态分布的参数的。假设均值为 100，方差为 4，则可以很容易算出最后一个小孩以 99%的概率低于 100 + 2 * 2.365 = 104.73 厘米。
参数模型与非参数模型 （parametric/non-parametric model） 上面两种方法中，第一种是非参数模型，它并不关心这些数据属于什么分布。第二种则是参数模型，其依赖于一个很强的先验假设：身高服从正态分布。换句话说，如果模型有一组固定的参数来描述其概率分布，则为参数模型；否则为非参数模型。
参数模型的优点是只有几个少量的参数就刻画了整个模型，每个参数都有其明确的统计意义（比如上面的均值和方差），但是它需要依赖于超强的先验假设：所有数据符合特定类型的概率分布。学习的过程就是利用训练数据估计未知参数的过程，这些估计出来的参数就是训练数据的浓缩。在这个过程中，先验知识确定了假设空间的取值范围，学习算法（比如最大似然估计或是最大后验概率估计）则在给定的范围内求解最优化问题。
可如果先验分布本身就不符合实际（换句话说你的假设空间错了），学习算法再强，数据再好也得不出正确结果。所以当对所要学习的问题知之甚少的时候，应当避免对潜在模型做出过多的假设。这类不使用先验信息，完全依赖数据进行学习得到的模型就是非参数模型。
非参数模型不是“无参数模型”，恰恰相反，非参数模型意味着模型参数的数目是不固定的，并且极有可能是无穷大，这决定了非参数模型不可能像参数模型那样用固定且有限数目的参数来完全刻画。在非参数模型中不存在关于数据潜在模式和结构化特性的任何假设，数据的所有统计特性都来源于数据本身，一切都是“所见即所得”。当训练数据趋于无穷多时，非参数模型可以逼近任意复杂的真实模型，而代价是非常大的时空复杂度。
归根结底，非参数模型其实可以理解为一种局部模型，就像战国时代每个诸侯国都有自己的国君一样，每个局部都有支配特性的参数。在局部上，相似的输入会得到相似的输出，而全局的分布就是所有局部分布的叠加。相比之下，参数模型具有全局的特性，所有数据都满足统一的全局分布，这就像履至尊而制六合得到的扁平化结构，一组全局分布的参数支配着所有的数据。
数据模型(data model)与算法模型(algorithm model) 从数据分布的角度看，不同的模型可以划分为参数模型和非参数模型两类。如果将这个划分标准套用到模型构造上的话，得到的结果就是数据模型（data model）和算法模型（algorithm model）。相比于参数对数据分布的刻画，这种分类方式更加侧重于模型对数据的拟合能力和预测能力。
http://www2.math.uu.se/~thulin/mm/breiman.pdf 统计模型，两种不同的文化。
作为一个统计学家，布雷曼看重的是学习算法从数据中获取有用结论和展示数据规律的能力。从这一点出发，他将从输入 xx 到输出 yy 的关系看成黑盒，数据模型认为这个黑盒里装着一组未知的参数，学习的对象是这组参数；算法模型则认为这个黑盒里装着一个未知的映射 f()˙f()˙，学习的对象也是这个映射。
数据模型和参数模型类似，其最典型的方法就是线性回归。线性回归的含义明确而清晰：输入数据每个单位的变化对输出都会产生同步的影响，影响的程度取决于这个特征的权重系数，不同特征对结果的贡献一目了然。
算法模型和非参数模型类似，其著名代表就是随机森林算法。随机森林是一种集成学习方法，构成这座森林的每一颗树都是决策树，每一棵决策树都用随机选取数据和待选特征构造出来，再按照少数服从多数的原则从所有决策树的结果中得到最终输出。
如果说参数模型与非参数模型的核心区别在于数据分布特征的整体性与局部性，那么数据模型和算法模型之间的矛盾就是模型的可解释性与精确性的矛盾。数据模型有更好的可解释性，而算法模型则相对而言抛弃了可解释性（对于特别复杂的问题，追求可解释性几乎是不可能的，追求精确性更重要）。
决策树本身是具有较好可解释性的数据模型，它表示的是几何意义上对特征空间的划分，但是精确度却不甚理想。随机森林解决了这个问题：通过综合使用建立在同一个数据集上的不同决策树达到出人意料的良好效果，在很多问题上都将精确度提升了数倍。但精确度的提升换来的是可解释性的下降。每个决策树对特征空间的单独划分共同织成一张剪不断理还乱的巨网，想要理解这张巨网背后的语义无异于水中望月、雾里看花。
生成模型与判别模型 还有另一种针对学习对象的划分方式，那就是生成模型和判别模型之分。简单地说，生成模型学习的对象是输入 xx 和输出 yy 的联合分布 p(x,y)p(x,y)，判别模型学习的则是已知输入 xx 的条件下，输出 yy 的条件分布 p(y|x)p(y|x)。两个分布可以通过贝叶斯定理建立联系。
生成模型和判别模型的区别可以这样来理解：假如我被分配了一个任务，要判断一个陌生人说的是什么语言。如果用生成模型来解决的话，我就需要把这个老外可能说的所有语言都学会，再根据他的话来判定语言的种类。但可能等我学完这些语言时，这个陌生人都说不出话了。可是用判别模型就简单多了，我只需要掌握不同语言的区别就足够了。即使不会西班牙语或者德语的任何一个单词，单凭语感也可以区分出这两种语言，这就是判别模型的优势。
针对生成模型和判别模型的利弊，支持向量机的奠基者弗拉基米尔·瓦普尼克（Vladimir Vapnik）有句名言：“（解决分类问题）应该直截了当，不要用兜圈子的方式，搞一个更难的问题（比如求解似然概率）做为中间步骤”。一般来说，生成模型的求解更加复杂，当数据量趋于无穷大时，渐进条件下的精确性也更差，但其收敛的速度更快，在较少数据的训练后就可以收敛到错误的下界。相比之下，判别模型的形式更加简单，在分类问题上的表现也更出色，却不能提供关于数据生成机制的信息。有些情况下，生成模型和判别模型会成对出现。例如在分类问题中，朴素贝叶斯和逻辑回归就是一对生成 - 判别分类器。"><meta property="og:type" content="article"><meta property="og:url" content="https://chenminhua.github.io/posts/2019_%E8%BA%AB%E9%AB%98%E8%83%8C%E5%90%8E%E7%9A%84%E7%BB%9F%E8%AE%A1%E5%AD%A6/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-03-08T20:00:08+08:00"><meta property="article:modified_time" content="2019-03-08T20:00:08+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="身高背后的统计学"><meta name=twitter:description content="给同年龄的一百个小孩量身高，已经量了 99 个，请问最后一个小孩的身高有 99%的概率会小于多少。
我们只知道这些数据来自同一个总体（服从同一个分布），前 99 个小孩和最后一个是一视同仁的，也就是说，这一百个小孩每个都有 1%的可能是最高的。所以最后那个小孩的身高有 99%的概率小于前面 99 个小孩中最高的那个。
而一些同学可能会有不同的观点，通常来说同龄小孩身高会服从正态分布。通过分析前 99 个小孩的身高我们是可以算出这个正态分布的参数的。假设均值为 100，方差为 4，则可以很容易算出最后一个小孩以 99%的概率低于 100 + 2 * 2.365 = 104.73 厘米。
参数模型与非参数模型 （parametric/non-parametric model） 上面两种方法中，第一种是非参数模型，它并不关心这些数据属于什么分布。第二种则是参数模型，其依赖于一个很强的先验假设：身高服从正态分布。换句话说，如果模型有一组固定的参数来描述其概率分布，则为参数模型；否则为非参数模型。
参数模型的优点是只有几个少量的参数就刻画了整个模型，每个参数都有其明确的统计意义（比如上面的均值和方差），但是它需要依赖于超强的先验假设：所有数据符合特定类型的概率分布。学习的过程就是利用训练数据估计未知参数的过程，这些估计出来的参数就是训练数据的浓缩。在这个过程中，先验知识确定了假设空间的取值范围，学习算法（比如最大似然估计或是最大后验概率估计）则在给定的范围内求解最优化问题。
可如果先验分布本身就不符合实际（换句话说你的假设空间错了），学习算法再强，数据再好也得不出正确结果。所以当对所要学习的问题知之甚少的时候，应当避免对潜在模型做出过多的假设。这类不使用先验信息，完全依赖数据进行学习得到的模型就是非参数模型。
非参数模型不是“无参数模型”，恰恰相反，非参数模型意味着模型参数的数目是不固定的，并且极有可能是无穷大，这决定了非参数模型不可能像参数模型那样用固定且有限数目的参数来完全刻画。在非参数模型中不存在关于数据潜在模式和结构化特性的任何假设，数据的所有统计特性都来源于数据本身，一切都是“所见即所得”。当训练数据趋于无穷多时，非参数模型可以逼近任意复杂的真实模型，而代价是非常大的时空复杂度。
归根结底，非参数模型其实可以理解为一种局部模型，就像战国时代每个诸侯国都有自己的国君一样，每个局部都有支配特性的参数。在局部上，相似的输入会得到相似的输出，而全局的分布就是所有局部分布的叠加。相比之下，参数模型具有全局的特性，所有数据都满足统一的全局分布，这就像履至尊而制六合得到的扁平化结构，一组全局分布的参数支配着所有的数据。
数据模型(data model)与算法模型(algorithm model) 从数据分布的角度看，不同的模型可以划分为参数模型和非参数模型两类。如果将这个划分标准套用到模型构造上的话，得到的结果就是数据模型（data model）和算法模型（algorithm model）。相比于参数对数据分布的刻画，这种分类方式更加侧重于模型对数据的拟合能力和预测能力。
http://www2.math.uu.se/~thulin/mm/breiman.pdf 统计模型，两种不同的文化。
作为一个统计学家，布雷曼看重的是学习算法从数据中获取有用结论和展示数据规律的能力。从这一点出发，他将从输入 xx 到输出 yy 的关系看成黑盒，数据模型认为这个黑盒里装着一组未知的参数，学习的对象是这组参数；算法模型则认为这个黑盒里装着一个未知的映射 f()˙f()˙，学习的对象也是这个映射。
数据模型和参数模型类似，其最典型的方法就是线性回归。线性回归的含义明确而清晰：输入数据每个单位的变化对输出都会产生同步的影响，影响的程度取决于这个特征的权重系数，不同特征对结果的贡献一目了然。
算法模型和非参数模型类似，其著名代表就是随机森林算法。随机森林是一种集成学习方法，构成这座森林的每一颗树都是决策树，每一棵决策树都用随机选取数据和待选特征构造出来，再按照少数服从多数的原则从所有决策树的结果中得到最终输出。
如果说参数模型与非参数模型的核心区别在于数据分布特征的整体性与局部性，那么数据模型和算法模型之间的矛盾就是模型的可解释性与精确性的矛盾。数据模型有更好的可解释性，而算法模型则相对而言抛弃了可解释性（对于特别复杂的问题，追求可解释性几乎是不可能的，追求精确性更重要）。
决策树本身是具有较好可解释性的数据模型，它表示的是几何意义上对特征空间的划分，但是精确度却不甚理想。随机森林解决了这个问题：通过综合使用建立在同一个数据集上的不同决策树达到出人意料的良好效果，在很多问题上都将精确度提升了数倍。但精确度的提升换来的是可解释性的下降。每个决策树对特征空间的单独划分共同织成一张剪不断理还乱的巨网，想要理解这张巨网背后的语义无异于水中望月、雾里看花。
生成模型与判别模型 还有另一种针对学习对象的划分方式，那就是生成模型和判别模型之分。简单地说，生成模型学习的对象是输入 xx 和输出 yy 的联合分布 p(x,y)p(x,y)，判别模型学习的则是已知输入 xx 的条件下，输出 yy 的条件分布 p(y|x)p(y|x)。两个分布可以通过贝叶斯定理建立联系。
生成模型和判别模型的区别可以这样来理解：假如我被分配了一个任务，要判断一个陌生人说的是什么语言。如果用生成模型来解决的话，我就需要把这个老外可能说的所有语言都学会，再根据他的话来判定语言的种类。但可能等我学完这些语言时，这个陌生人都说不出话了。可是用判别模型就简单多了，我只需要掌握不同语言的区别就足够了。即使不会西班牙语或者德语的任何一个单词，单凭语感也可以区分出这两种语言，这就是判别模型的优势。
针对生成模型和判别模型的利弊，支持向量机的奠基者弗拉基米尔·瓦普尼克（Vladimir Vapnik）有句名言：“（解决分类问题）应该直截了当，不要用兜圈子的方式，搞一个更难的问题（比如求解似然概率）做为中间步骤”。一般来说，生成模型的求解更加复杂，当数据量趋于无穷大时，渐进条件下的精确性也更差，但其收敛的速度更快，在较少数据的训练后就可以收敛到错误的下界。相比之下，判别模型的形式更加简单，在分类问题上的表现也更出色，却不能提供关于数据生成机制的信息。有些情况下，生成模型和判别模型会成对出现。例如在分类问题中，朴素贝叶斯和逻辑回归就是一对生成 - 判别分类器。"><link rel=stylesheet type=text/css media=screen href=https://chenminhua.github.io/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://chenminhua.github.io/css/main.css><link id=dark-scheme rel=stylesheet type=text/css href=https://chenminhua.github.io/css/dark.css><script src=https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js></script>
<script src=https://chenminhua.github.io/js/main.js></script></head><body><div class="container wrapper"><div class=header><div class=avatar><a href=https://chenminhua.github.io/><img src="https://avatars.githubusercontent.com/u/10234400?v=4" alt=硕大的汤姆></a></div><h1 class=site-title><a href=https://chenminhua.github.io/>硕大的汤姆</a></h1><div class=site-description><p>The official website of Minhua Chen</p><nav class="nav social"><ul class=flat><li><a href=https://github.com/knadh/hugo-ink title=Github><i data-feather=github></i></a></li><li><a href=/index.xml title=RSS><i data-feather=rss></i></a></li></ul></nav></div><nav class=nav><ul class=flat><li><a href=/>Home</a></li><li><a href=/posts>All posts</a></li><li><a href=/about>About</a></li><li><a href=/algo>Algo</a></li><li><a href=/principles>原则</a></li><li><a href=/tags>Tags</a></li></ul></nav></div><div class=post><div class=post-header><div class=meta><div class=date><span class=day>08</span>
<span class=rest>Mar 2019</span></div></div><div class=matter><h1 class=title>身高背后的统计学</h1></div></div><div class=markdown><p>给同年龄的一百个小孩量身高，已经量了 99 个，请问最后一个小孩的身高有 99%的概率会小于多少。</p><p>我们只知道这些数据来自同一个总体（服从同一个分布），前 99 个小孩和最后一个是一视同仁的，也就是说，这一百个小孩每个都有 1%的可能是最高的。所以最后那个小孩的身高有 99%的概率小于前面 99 个小孩中最高的那个。</p><p>而一些同学可能会有不同的观点，通常来说同龄小孩身高会服从正态分布。通过分析前 99 个小孩的身高我们是可以算出这个正态分布的参数的。假设均值为 100，方差为 4，则可以很容易算出最后一个小孩以 99%的概率低于 100 + 2 * 2.365 = 104.73 厘米。</p><h2 id=参数模型与非参数模型-parametricnon-parametric-model>参数模型与非参数模型 （parametric/non-parametric model）</h2><p>上面两种方法中，第一种是非参数模型，它并不关心这些数据属于什么分布。第二种则是参数模型，其依赖于一个很强的先验假设：身高服从正态分布。换句话说，如果模型有一组固定的参数来描述其概率分布，则为参数模型；否则为非参数模型。</p><p>参数模型的优点是只有几个少量的参数就刻画了整个模型，每个参数都有其明确的统计意义（比如上面的均值和方差），但是它需要依赖于超强的先验假设：所有数据符合特定类型的概率分布。学习的过程就是利用训练数据估计未知参数的过程，这些估计出来的参数就是训练数据的浓缩。在这个过程中，先验知识确定了假设空间的取值范围，学习算法（比如最大似然估计或是最大后验概率估计）则在给定的范围内求解最优化问题。</p><p>可如果先验分布本身就不符合实际（换句话说你的假设空间错了），学习算法再强，数据再好也得不出正确结果。所以当对所要学习的问题知之甚少的时候，应当避免对潜在模型做出过多的假设。这类不使用先验信息，完全依赖数据进行学习得到的模型就是非参数模型。</p><p>非参数模型不是“无参数模型”，恰恰相反，非参数模型意味着模型参数的数目是不固定的，并且极有可能是无穷大，这决定了非参数模型不可能像参数模型那样用固定且有限数目的参数来完全刻画。在非参数模型中不存在关于数据潜在模式和结构化特性的任何假设，数据的所有统计特性都来源于数据本身，一切都是“所见即所得”。当训练数据趋于无穷多时，非参数模型可以逼近任意复杂的真实模型，而代价是非常大的时空复杂度。</p><p>归根结底，非参数模型其实可以理解为一种局部模型，就像战国时代每个诸侯国都有自己的国君一样，每个局部都有支配特性的参数。在局部上，相似的输入会得到相似的输出，而全局的分布就是所有局部分布的叠加。相比之下，参数模型具有全局的特性，所有数据都满足统一的全局分布，这就像履至尊而制六合得到的扁平化结构，一组全局分布的参数支配着所有的数据。</p><h2 id=数据模型data-model与算法模型algorithm-model>数据模型(data model)与算法模型(algorithm model)</h2><p>从数据分布的角度看，不同的模型可以划分为参数模型和非参数模型两类。如果将这个划分标准套用到模型构造上的话，得到的结果就是数据模型（data model）和算法模型（algorithm model）。相比于参数对数据分布的刻画，这种分类方式更加侧重于模型对数据的拟合能力和预测能力。</p><p><a href=http://www2.math.uu.se/~thulin/mm/breiman.pdf>http://www2.math.uu.se/~thulin/mm/breiman.pdf</a> 统计模型，两种不同的文化。</p><p>作为一个统计学家，布雷曼看重的是学习算法从数据中获取有用结论和展示数据规律的能力。从这一点出发，他将从输入 xx 到输出 yy 的关系看成黑盒，数据模型认为这个黑盒里装着一组未知的参数，学习的对象是这组参数；算法模型则认为这个黑盒里装着一个未知的映射 f()˙f()˙，学习的对象也是这个映射。</p><p>数据模型和参数模型类似，其最典型的方法就是线性回归。线性回归的含义明确而清晰：输入数据每个单位的变化对输出都会产生同步的影响，影响的程度取决于这个特征的权重系数，不同特征对结果的贡献一目了然。</p><p>算法模型和非参数模型类似，其著名代表就是随机森林算法。随机森林是一种集成学习方法，构成这座森林的每一颗树都是决策树，每一棵决策树都用随机选取数据和待选特征构造出来，再按照少数服从多数的原则从所有决策树的结果中得到最终输出。</p><p>如果说参数模型与非参数模型的核心区别在于数据分布特征的整体性与局部性，那么数据模型和算法模型之间的矛盾就是模型的可解释性与精确性的矛盾。数据模型有更好的可解释性，而算法模型则相对而言抛弃了可解释性（对于特别复杂的问题，追求可解释性几乎是不可能的，追求精确性更重要）。</p><p>决策树本身是具有较好可解释性的数据模型，它表示的是几何意义上对特征空间的划分，但是精确度却不甚理想。随机森林解决了这个问题：通过综合使用建立在同一个数据集上的不同决策树达到出人意料的良好效果，在很多问题上都将精确度提升了数倍。但精确度的提升换来的是可解释性的下降。每个决策树对特征空间的单独划分共同织成一张剪不断理还乱的巨网，想要理解这张巨网背后的语义无异于水中望月、雾里看花。</p><h2 id=生成模型与判别模型>生成模型与判别模型</h2><p>还有另一种针对学习对象的划分方式，那就是生成模型和判别模型之分。简单地说，生成模型学习的对象是输入 xx 和输出 yy 的联合分布 p(x,y)p(x,y)，判别模型学习的则是已知输入 xx 的条件下，输出 yy 的条件分布 p(y|x)p(y|x)。两个分布可以通过贝叶斯定理建立联系。</p><p>生成模型和判别模型的区别可以这样来理解：假如我被分配了一个任务，要判断一个陌生人说的是什么语言。如果用生成模型来解决的话，我就需要把这个老外可能说的所有语言都学会，再根据他的话来判定语言的种类。但可能等我学完这些语言时，这个陌生人都说不出话了。可是用判别模型就简单多了，我只需要掌握不同语言的区别就足够了。即使不会西班牙语或者德语的任何一个单词，单凭语感也可以区分出这两种语言，这就是判别模型的优势。</p><p>针对生成模型和判别模型的利弊，支持向量机的奠基者弗拉基米尔·瓦普尼克（Vladimir Vapnik）有句名言：“（解决分类问题）应该直截了当，不要用兜圈子的方式，搞一个更难的问题（比如求解似然概率）做为中间步骤”。一般来说，生成模型的求解更加复杂，当数据量趋于无穷大时，渐进条件下的精确性也更差，但其收敛的速度更快，在较少数据的训练后就可以收敛到错误的下界。相比之下，判别模型的形式更加简单，在分类问题上的表现也更出色，却不能提供关于数据生成机制的信息。有些情况下，生成模型和判别模型会成对出现。例如在分类问题中，朴素贝叶斯和逻辑回归就是一对生成 - 判别分类器。</p></div><div class=tags></div><div class=back><a href=https://chenminhua.github.io/><span aria-hidden=true>← Back</span></a></div><div class=back></div></div></div><div class="footer wrapper"><nav class=nav><div>2019</div></nav></div><script>feather.replace()</script></body></html>